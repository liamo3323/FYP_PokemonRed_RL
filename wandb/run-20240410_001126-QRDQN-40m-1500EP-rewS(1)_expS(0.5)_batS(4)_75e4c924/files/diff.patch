diff --git a/LaTeX Work/8_Conclusion.tex b/LaTeX Work/8_Conclusion.tex
index 0165cb3..7fd581e 100644
--- a/LaTeX Work/8_Conclusion.tex	
+++ b/LaTeX Work/8_Conclusion.tex	
@@ -14,10 +14,14 @@ Something else that helped with the most time consuming aspect of the research p
 
 \subsection{Challenges}
 
-- The reward function was difficult to implement and required a lot of trial and error to get right
-- Number of timesteps for the experiment was difficult 
-- Steps per episode 
-- Getting the agent to balance navigation and battling 
+While conducting this research, a quite a few issues arose that made the project more difficult to complete but were still able to be solved. The largest issue and main objective of this research was to find the correct reward function so that it was feasible for the agent to reach the goal of the firsy gym in order to compare their performances. The reward function of the multi-objective environment was difficult to implement correctly by balancing battling and navigation. The correct values were eventually discovered after countless agents being trained and tested, all of which can be found in the evaluation section of the report. However, being able to train an agent once during the day and once during the night sped up the process. 
+
+The difficulty of tuning the reward function was contributed by the inital reward function that was provided by the environment. The environment had very simple rewards that were not enough to train an agent to complete the first gym, it was deigned to constantly train a singular agent till it eventually reached the goal. For example, the reward to encourage battling was static and would stop increasing the amount of reward given to the agent after reaching a certain level, which would result in the agent stopping to battle after reaching a certain point in the game. Another example was the reward for navigation, which would give a reward for ever new screen. However, this could be exploited by the agent by navigating through menus and not actually moving the character in the game. Instead, reward would only be given when a new screen is experienced and coordinates are updated. 
+
+In addition, finding the correct number of total timesteps for the experiment was also difficult because it needed the value function to be corrected before further experimentation on an appropriate number of timesteps could be conducted. Initally, a value of 20 million timesteps was chosen, but was later increased to 40 million to ensure that the agents had enough time to learn the environment and reach the first gym more consistently. However, it was a time consuming task and required trial and error and lots of training time.
+
+Another challenge that was faced during the research project was getting the inital algorithm script to work correctly. There were multiple examples of the algorithms being implemented in the stablebaselines3 library working with gym environments, however, the environment for this research was a custom gym environment and had to support multiple agents training on multiple instances of the environment, which is an added challenge. 
+
 - The amount of time it took to determine the effectiveness of reward values
 
 \subsection{Improvements}
diff --git a/run_baseline_QRDQN.py b/run_baseline_QRDQN.py
index 1114ac3..786cf9d 100644
--- a/run_baseline_QRDQN.py
+++ b/run_baseline_QRDQN.py
@@ -66,6 +66,10 @@ if __name__ == "__main__":
     learn_steps = 32
     sess_id = str(uuid.uuid4())[:8]
 
+    reward_scale = 1
+    explore_weight = 0.5
+    battle_weight = 4
+
     sess_path = Path(f'Sessions/{algorithm}_Session_{current_datetime_str}_{sess_id}_env2_1')
     # sess_path = Path(f'Sessions/PPO_Session_0307161249_7602f77b_env2_2')
     print(sess_path)
@@ -103,8 +107,8 @@ if __name__ == "__main__":
         import wandb
         from wandb.integration.sb3 import WandbCallback
         run = wandb.init(
-            project="pokemon-red-train",
-            id=sess_id,
+            project="FYP-RL-DATA",
+            id=f"{algorithm}-{mil_timestep}m-{episode_length_per_cpu}EP-rewS({reward_scale})_expS({explore_weight})_batS({battle_weight})_{sess_id}", #PPO-40m-1500EP-rewS:1.0_expS:0.5_batS:4
             config=env_config,
             sync_tensorboard=True,  
             monitor_gym=True,  
@@ -112,6 +116,7 @@ if __name__ == "__main__":
         )
         callbacks.append(WandbCallback())
 
+
     # put a checkpoint here you want to start from
     # file_name = f"Sessions/PPO_Session_0307161249_7602f77b_env2_1/poke_4235000_steps"
     file_name = f"blank"
diff --git a/run_baseline_v2.py b/run_baseline_v2.py
index 8de861c..3424ef9 100644
--- a/run_baseline_v2.py
+++ b/run_baseline_v2.py
@@ -43,7 +43,7 @@ def make_model(algorithm):
 
     elif algorithm == ("A2C"):
         model = A2C('MultiInputPolicy', env, verbose=1,
-                    n_steps=ep_length, gamma=gamma, tensorboard_log=sess_path)
+                    n_steps=ep_length, gamma=gamma, tensorboard_log=sess_path, non_blocking=False, pin_memory=False)
 
     elif algorithm == ("DQN"):
         model = DQN('MultiInputPolicy', env, verbose=1, gamma=gamma, tensorboard_log=sess_path)
@@ -61,7 +61,7 @@ def make_model(algorithm):
 if __name__ == "__main__":
     current_datetime_str = datetime.datetime.now().strftime("%m%d%H%M%S")
     use_wandb_logging = True
-    algorithm = "ARS"
+    algorithm = "A2C"
     batch_size = 64 #! was 128
     gamma = 0.998
     n_epochs = 3
