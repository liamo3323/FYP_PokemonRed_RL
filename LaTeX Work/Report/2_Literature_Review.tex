\section{Literature Review}

List of things to cover:

- justify research: -> what is pkmn and RPG games \& why did I chose pkmn red

- up-to-date with relevant literature: -> the algorithms I plan on using and why?

\subsection{Introduction to Pokemon Red}

Pokémon Red is a role-playing game where the player's goal is to defeatt the eight Gym Leaders and the Elite Four. The game is played from a top-down perspective and the player controls the actions of the main character. The player has to navigate the main character around the overworld, interact with non-player characters, and battle with wild Pokémon and other trainers. The game's combat is a turn-based battle system, where the player selects moves for their Pokémon to perform in battle. The game has a large number of items, moves, and abilities that the player can use to their advantage in battle. Therefore, large amount of variance makes each playthrough of the game unique while keeping goals consistent for each play through.

\subsection{Why Pokemon Red?}

RL has only been applied to every Atari game, where it surpassed the human benchmark for every game \cite{brockman2016openai}. However, these games lack long term randomness, where present actions influencing future states and future decisions. Every pokemon game takes at least 30 hours to complete, where every tiny decision made at every minute has an influence on the future. An example of this would be the decision of which pokemon to catch and train or which pokemon you start you journey with. The agent cannot defeat the game if it is able to navigate the map confidently but avoid battling, and on the other hand, the agent cannot complete the game if it chooses to only battle and not explore the map.
 
In addition, other similar projects exist that apply reinforcement learning to pokemon to find the optimal battling strategy by Kalose et al \cite{kalose2018optimal}. 
Their work focuses on the pokemon battling combat aspect of the game. However, I believe that it does not have a large enough search space to justify application of RL techniques because applying the same set of actions will always lead to very similar states, therefore making it possible for a rule based algorithm to perform pokemon battling optimally. 
Another similar work by Flaherty, Jimenez and Abbasi \cite{flaherty2021playing} applies reinforcement learning algorithms A2C and DQN to play Pokémon Red. However, this piece of work explores the time it takes to traini the different agents using RAM observations versus image observations. I aim to extend their research by applying and comparing more algorithms and various RL techniques to find the optimal path to complete the game.

\subsection{Why choose RL?}

Within the field of machine learning, there are multiple different forms of learning that can be applied to the environment of pokemon red. However, due to the depth of the action space of the environment, reinforcement learning is the best form of machine learning to explore how to find the optimal path to completing the game. 

The biggest drawback when using supervised or unsupervised learning is the dataset. The dataset would need someone playing the game for countless hours completing the game or reaching a checkpoint in the game before starting another episode of playing the game to provide a more varied dataset. Not only is this method increcible slow, as humans can only play and operate at a certain speed, but the dataset would also never experience actions that are unnatural for a human to perform, as the dataset is bound to the actions a human would take with the human's preconception of how to play the game. This leads onto the other issue, where the dataset of human playing the game is bound by human constaints. Humans are naturally lazy and have short attention spans, which means when the person providing the training data knows one way to solve a puzzle, they are unlikely to experiment other methods in solving the puzzle. Therefore, the agents trained on the human provided data are bound and limited by the performance of the human and will never find a more optimal path. 

RL is the solution to finding the set of action to complete the game as it allows the agent to 'play' the game itself and explore the environment in a sped-up space to find the optimal path without the need of a human to show it how to play, while knowing what the goal is. 

\subsection{Proximal Policy Optimization (PPO)}

\subsection{Deep Q Networks (DQN)}

DQN was developed developed by combining reinforcement learning techniques and deep neural networks at scale by enhancing the Q-Learning algorithm with deep neural networks and a technique called experience replay \cite{TFAgentsAuthors2023}. DQN is a value based algorithms that attempts to learn the environment by optimizing a value function, where it expects a high amount of value to be returned in the long term \cite{deepcheckRL}. Therefore, the learned agent will maximise future reward by making assumptions of rewards of future states yet to be experienced. This is an important step in value-based algorithms, as optimal action-value functions obey an important identity known as the Bellman equation \cite{mnih2013playing}. 

Q-Learning is based on the Bellman equation, where the agent learns the optimal action-value function by iteratively updating the Q-values of the state-action pairs \cite{mnih2013playing}. The Q-value of a state-action pair is the expected return of taking the action in the current state, where the value of a state as the sum of the immediate reward and the discounted value of the future states \cite{bellman1958dynamic}.  However, it is impractical to explore the Q-values of every state-action pair in a table, as the state space of the environment is too large to store in memory \cite{mnih2013playing}. Instead, a function approximator is used to estimate the Q-values. Q-learning is an off-policy algorithm, meaning that is has a seperate behavior policy for interacting with the environment. In DQN, the behavior policy is an epsilon-greedy policy, where the agent selects the action with the highest Q-value with probability 1 - epsilon and selects a random action with probability epsilon with the intent of finding a better action \cite{TFAgentsAuthors2023}.

Despite value based algorithms theoretically being able to find the optimal policy, value based algorithms have a few weaknesses when deployed for real world use outside of the simulated environment. One weakness value based algorithms have is the ability to generalise to new situations beyond the environment \cite{OdelTruxillo2023}. This is considered the reality gap, where there is a mismatch between the training environment of the policy and the real-world environment in which the trained agent is deployed in \cite{tobin2017domain}. Transfer learning is a technique that can be used to mitigate the reality gap, where the agent is trained in a simulated environment and then transferred to the real world environment \cite{OdelTruxillo2023}. However, transfer learning is not always possible, as the real world environment may be too complex to simulate accurately as the real-world is constantly changing and dynamic and most environments are static and stochastic \cite{OdelTruxillo2023}. 

Another weakness of value based algorithms is the environment they are trained in. Value based algorithms are able to converge towards the optimal policy by finding the set of actions which reward the most amount of value \cite{OdelTruxillo2023}. However, the environment the agent is trained in must be fully observable, where the agent is able to see the entire state of the environment at every timestep. This is a weakness and dependency on complete complete and whole information, as most real world environments are partially observable, where the agent is unable to see the entire state of the environment at every time step and would be forced to make informed decisions on partial information \cite{dulac2021challenges}. 

\subsection{Advantage Actor-Critic (A2C)}

A2C is an algorithm that combines aspects of policy gradient algorithms and value-based methods, where the policy (actor) selects actions and the value function, critic, is trained to estimate the expected reward \cite{mnih2013playing}. Value-based Algorithms, such as DQN, attempted to approximate the optimal value fucntion for a set of action and expected value (cite AI SUmmer ActorCritic). 


\subsection{Algorithm Choice}

The algorithms I plan on using are Proximal Policy Optimization (PPO), Advanrage Actor-Critic (A2C), and Deep Q Networks(DQN). These algorithms are commonly used algorithms used in the field of reinforcement learning for research purposes. The reason why I chose these algorithms is because my environment holds a discrete action space, where each action input is mapped to a possible button press on the gameboy.
