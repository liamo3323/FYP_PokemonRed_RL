\section{Literature Review}

List of things to cover:

- justify research: -> what is pkmn and RPG games \& why did I chose pkmn red

- up-to-date with relevant literature: -> the algorithms I plan on using and why?

\subsection{Introduction to Pokemon Red}

Pokémon Red is a role-playing game where the player's goal is to defeatt the eight Gym Leaders and the Elite Four. The game is played from a top-down perspective and the player controls the actions of the main character. The player has to navigate the main character around the overworld, interact with non-player characters, and battle with wild Pokémon and other trainers. The game's combat is a turn-based battle system, where the player selects moves for their Pokémon to perform in battle. The game has a large number of items, moves, and abilities that the player can use to their advantage in battle. Therefore, large amount of variance makes each playthrough of the game unique while keeping goals consistent for each play through.

\subsection{Why Pokemon Red?}

RL has only been applied to every Atari game, where it surpassed the human benchmark for every game \cite{brockman2016openai}. However, these games lack long term randomness, where present actions influencing future states and future decisions. Every pokemon game takes at least 30 hours to complete, where every tiny decision made at every minute has an influence on the future. An example of this would be the decision of which pokemon to catch and train or which pokemon you start you journey with. The agent cannot defeat the game if it is able to navigate the map confidently but avoid battling, and on the other hand, the agent cannot complete the game if it chooses to only battle and not explore the map.
 
In addition, other similar projects exist that apply reinforcement learning to pokemon to find the optimal battling strategy by Kalose et al \cite{kalose2018optimal}. Their work focuses on finding an optimal pokemon battling strategy using model-free reinforcement learning strategies \cite{kalose2018optimal}. Not only does their work only focus on one aspect of the pokemon game which turn-based battling in video games is present in many other video games outside of the pokemon franchise, but also only focuses on researching different model-free strategies. I believe that the environment the research was conducted does not have a large enough search space and is not stochastic enough to justify application of RL techniques. This is because applying the same action to a set state between different episodes almost always lead to the same state, therefore making it possible for a rule based algorithm to optimize pokemon battling. 

Another similar work by Flaherty, Jimenez and Abbasi \cite{flaherty2021playing} applies reinforcement learning algorithms A2C and DQN to play Pokémon Red. Their work focuses on comparing A2C to DQN to see if RAM observations was better than image observations for training agents \cite{flaherty2021playing}. Their work is  closely related to what I am to achieve with this project, as I am also comparing the performance of A2c and DQN. However, my research aims to combine RAM and image observations during training to determine which style of algorithm is best suited for multi-objective reinforcement learning. I aim to extend their research by applying and comparing more algorithms and various RL techniques to find the optimal method to complete the game.

\subsection{Why choose RL?}

Within the field of machine learning, there are multiple different forms of learning that can be applied to the environment of pokemon red. However, due to the depth of the action space of the environment, reinforcement learning is the best form of machine learning to explore how to find the optimal path to completing the game. 

The biggest drawback when using supervised or unsupervised learning is the dataset. The dataset would need someone playing the game for countless hours completing the game or reaching a checkpoint in the game before starting another episode of playing the game to provide a more varied dataset \cite{XanderSteenbrugge2019intro}. Not only is this method increcible slow, as humans can only play and operate at a certain speed, but the dataset would also never experience actions that are unnatural for a human to perform, as the dataset is bound to the actions a human would take with the human's preconception of how to play the game. This leads onto the other issue, where the dataset of human playing the game is bound by human constaints. Humans are naturally lazy and have short attention spans, which means when the person providing the training data knows one way to solve a puzzle, they are unlikely to experiment other methods in solving the puzzle \cite{XanderSteenbrugge2019intro}. Therefore, the agents trained on the human provided data are bound and limited by the performance of the human and will never find a more optimal path. 

RL is the solution to finding the set of action to complete the game as it allows the agent to 'play' the game itself and explore the environment in a sped-up space to find the optimal path without the need of a human to show it how to play, while knowing what the goal is. 

\subsection{Proximal Policy Optimization (PPO)}

- Need to explain policy based algorithms 
- Need to explain advantage 
- Need to explain on policy and off policy 

Reinforcement learning's training data is dependent on the policy generating its own training dataset, which is dependent on the training of its policy to provide valuable data \cite{XanderSteenbrugge2019ppo}. This means that the distribution of observations and training is constantly changing as the policy is constantly being updated, which brings about instability during training. Therefore, there is a need to stabilize the policy updates during training to avoid the a high learning rate to push the policy into a space where the next batch of data is learned under a poor policy further destabilizing the policy \cite{XanderSteenbrugge2019ppo}.

\begin{quote}
    The idea with Proximal Policy Optimization (PPO) is that we want to improve the training stability of the policy by limiting the change you make to the policy at each training epoch: we want to avoid having too large of a policy update.
    \end{quote}
    \hspace*{\fill} \textit{Thomas Simonini, 2022}

PPO aims to keep the data efficiency and reliable of algorithm TRPO, while only using first-order optimizations. The soltion to this problem was to make pessimistic estimations of the performance of the policy \cite{schulman2017proximal}. By limiting the amount the policy can update by per epoch or episode, it can stabilize the policy during training. Policy optimization is done by alternating between samplinig data from the policy and performing multiple epochs of optimization on the sampled data \cite{schulman2017proximal}. 

PPO is a policy based algorithm, where the policy is trained to get the most reward in the future by doing actions in each state \cite{deepcheckRL}. In comparison to value based algorithms, policy based algorithms are better at converging towards the optimum policy within stochastic environments \cite{mnih2015human}. 

Policy Gradient methods work by estimating the policy gradient  
%! https://www.youtube.com/watch?v=5P7I-xPq8u8 XanderSteenbrugge2019ppo 

\subsection{Deep Q Networks (DQN)}

DQN was developed developed by combining reinforcement learning techniques and deep neural networks at scale by enhancing the Q-Learning algorithm with deep neural networks and a technique called experience replay \cite{TFAgentsAuthors2023}. DQN is a value based algorithms that attempts to learn the environment by optimizing a value function, where it expects a high amount of value to be returned in the long term \cite{deepcheckRL}. Therefore, the learned agent will maximise future reward by making assumptions of rewards of future states yet to be experienced. This is an important step in value-based algorithms, as optimal action-value functions obey an important identity known as the Bellman equation \cite{mnih2013playing}. 

Q-Learning is based on the Bellman equation, where the agent learns the optimal action-value function by iteratively updating the Q-values of the state-action pairs \cite{mnih2013playing}. The Q-value of a state-action pair is the expected return of taking the action in the current state, where the value of a state as the sum of the immediate reward and the discounted value of the future states \cite{bellman1958dynamic}.  However, it is impractical to explore the Q-values of every state-action pair, as the state space of the environment is too large to computationally check \cite{mnih2013playing}. Instead, a function approximator is used to estimate the Q-values. Q-learning is an off-policy algorithm, meaning that is has a seperate behavior policy for interacting with the environment. In DQN, the behavior policy is an epsilon-greedy policy, where the agent selects the action with the highest Q-value with probability 1 - epsilon and selects a random action with probability epsilon with the intent of finding a better action \cite{TFAgentsAuthors2023}.

Despite value based algorithms theoretically being able to find the optimal policy, value based algorithms have a few weaknesses when deployed for real world use outside of the simulated environment. One weakness value based algorithms have is the ability to generalise to new situations beyond the environment \cite{OdelTruxillo2023}. This is considered the reality gap, where there is a mismatch between the training environment of the policy and the real-world environment in which the trained agent is deployed in \cite{tobin2017domain}. Transfer learning is a technique that can be used to mitigate the reality gap, where the agent is trained in a simulated environment and then transferred to the real world environment \cite{OdelTruxillo2023}. However, transfer learning is not always possible, as the real world environment may be too complex to simulate accurately as the real-world is constantly changing and dynamic and most environments are static and stochastic \cite{OdelTruxillo2023}. 

Another weakness of value based algorithms is the environment they are trained in. Value based algorithms are able to converge towards the optimal policy by finding the set of actions which reward the most amount of value \cite{OdelTruxillo2023}. However, the environment the agent is trained in must be fully observable, where the agent is able to see the entire state of the environment at every timestep. This is a weakness and dependency on complete complete and whole information, as most real world environments are partially observable, where the agent is unable to see the entire state of the environment at every time step and would be forced to make informed decisions on partial information \cite{dulac2021challenges}. 

\subsection{Advantage Actor-Critic (A2C)}

A2C is an algorithm that combines aspects of policy gradient algorithms and value-based methods, where the policy (actor) selects actions and the value function, critic, is trained to estimate the expected reward \cite{mnih2013playing}. The actor follows a policy-based learning and takes the state as input and outputs the best possible action. This actor policy controls how the agents behaves by learning the optimal policy \cite{SergiosKaragiannakos2018}. The critic evaluates the selected action by the policy by computing its value using the value function. These two aspects of the algorithm get better doing their role by learning from each other and operate better together than as two methods seperately \cite{SergiosKaragiannakos2018}. 

As we learnt in DQN, value-based algorithms often use Q values to determine the best action to take in a given state. However, A2C uses the advantage function to determine the best action to take in a given state. Due to how Q values can be decomposed into the value function and the advantage function, the advantage value can be calculated to determine how good it is to be in the current state. \cite{SergiosKaragiannakos2018}. In other words, the advantage function calculates the extra reward recieved if the agent takes the action in the current state \cite{ThomasSimonini2022A2C}.

One advantage of combining value-based and policy-based algorithms by using Actor-Critic methods, is to help with stablize training and reduce the a mount of variance \cite{SergiosKaragiannakos2018}. When training a model in reinforcement learning, the aim is to increase the probability of actions in a trajectory proportion to how high the return is. Such that if the returns a high, we will push the probability of the state action pair and if the the return is low, we will decrease the probability of the state action pair. The return value is calculated using Monte-Carlo sampling, where the agent trajectory is calculated using the discounted return to determine if the probability of the state action should increase or decrease. The issue with this is that in a stochastic environment, the return from the same state in different episodes can be significantly different \cite{ThomasSimonini2022A2C}. Actor critic methods help reduce the amount of variance by utilizing techniques such as clipped delayed policy updates and clipped double q learning \cite{padhye2023deep}.

\subsection{Algorithm Choice}

The algorithms I plan on using are Proximal Policy Optimization (PPO), Advanrage Actor-Critic (A2C), and Deep Q Networks(DQN). These algorithms are commonly used algorithms used in the field of reinforcement learning for research purposes. The reason why I chose these algorithms is because my environment holds a discrete action space, where each action input is mapped to a possible button press on the gameboy.

Policy based algorithms converge towards the optimal policy faster for continious and stochastic environments, while value based algorithms are more sample efficient and steady \cite{SergiosKaragiannakos2018}.

(Comparison of On-Policy Deep Reinforcement Learning A2C with Off-Policy DQN in Irrigation Optimization: A Case Study at a Site in Portugal <-- can be a good reference to find the differences between the two algorithms)

compare on-policy and off-policy algorithm (DQN vs A2C)

On-policy algortihms evaluate adn improve its policy while using the same policy to make decisions on actions taken. Off-policy algorithms have a seperate behaviour policy from its action selection policy, where the behavior policy learns from experiences generated (find a source)