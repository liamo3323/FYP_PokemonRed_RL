\section{Introduction}

The aim of this project is to use reinforcement learning techniques to solve a complex multi-objective environment without the use of multiple agents. The goal is to train an agent using different algorithms ability to compare their ability to navigate and complete the game 'Pokémon Red' with the least amount of timesteps. The problem space, Pokémon Red, was chosen due it being a complex environment with an unfathamable search space, while also being a game that's target audience is for children.

\subsection{Aims of the Project}

With this project I am to train multiple agents using different reinforcement learning algorithms to compare each of their performance in the multi-objective environments. These agents will be hyperparameter tuned to ensure that each algorithm is performing at its best to the environment. The agents trained on differnet algorithms will be trained for a set number of timesteps and then evaluated on their ability to solve the goal of the environment. I will be evaluating each model by comparing the cumulative reward, learning speed and stability \cite{Sutton1}. 

Within the environment of pokemon red, the goal of the environment is to complete the game. However, due to the complexity of the game, minimum length of 25+ hours requried to complete the game and large search space, a smaller more achieveable goal was set. The goal of the agent for this research paper is to collect the first of eight gym badge with the least number of timesteps. Within the game, collecting pokemon badge is an essential step to completing the game, which is why it is a good guage of the models potential to complete the game. Despite this scaled down goal, the agents will still need to learn to balance both navigate within the environment and learn to battle in order to defeat the first gym leader.

\subsection{What is Reinforcement Learning}

Decision-making is a recurring activity that every individual faces in their everyday life, leading to short- or long-term consequences with differing levels of satisfaction. When making decisions in uncertain environments, humans have the ability to take previous experiences and apply them to new environments to make decisions grounded in knowledge. This is called sequential decision-making. In the context of machine learning, it refers to a decision-making agent in an observation and action loop, where after a series of actions, its performance can be measured \cite{francon2020effective}. Taking inspiration from biological learning systems, reinforcement learning is the closest kind of machine learning that learns using observed data to influence the brain's reward system \cite{Sutton1}. 

Reinforcement learning is different from traditional forms of machine learning techniques as it learns "how to map situations maximize a compare their ability to numerical reward signal \cite{Sutton1}". Traditional machine that     it is provided, while reinforcement learning is dependent on the environment that it is trained on. Another feature of reinforcement learning other traditional learning methods lack, is continious learning \cite{sreenivas2022safe}. While other forms of machine learning are deployed and ready to be applied for their designed use, it is following a static set of rules that it has learnt. Reinforcement learning also follows a similar set of rules, however, it is still learing even when on the field. Therefore, it is possible for reinforcement learning to better make minor changes and adapt to the differences between what is set in the environment and the real deployed situation \cite{sreenivas2022safe}.

One way of understanding how reinforcement learning agents learn to solve a problem from interaction is via markov decision processes \cite{Sutton1}.

\begin{quote}
MDPs are a classical formalization of sequential decision making,
where actions influence not just immediate rewards, but also subsequent situations, or
states, and through those future rewards. Thus MDPs involve delayed reward and the
need to tradeoff immediate and delayed reward.
\end{quote}
\hspace*{\fill} \textit{Sutton, 2018, p47.}

Agents' requirement to learn through experience and actions performed on current states not only affect the present but also affect future states and actions. Initially, the agent has no understanding of the environment; however, through random action selection and the reward that it receives, it learns an understanding of the environment it is in. In reinforcement learning, the agent is an AI that chooses actions by following a set of rules; this set of rules is called a policy. The action chosen by the policy is applied to the environment. The place in which the decision-making agent performs actions and the entity that determines what is right and wrong are called the environment. The environment is a simulation of the world, which reflects the task the agent aims to solve. After the action has been applied to the environment, the change in the environment will be evaluated and return positive or negative feedback to the agent. This positive or negative feedback is called reward value. If the action chosen by the agent satisfied the goal of the environment, a positive reward would be returned. The agent’s aim is to maximize the amount of positive reward it can receive. Therefore, over a long period of time, the agent will learn the set of actions that lead to the highest accumulative reward, which should solve the problem presented in the environment. 

\subsection{Elements of Reinforcement Learning}

Other than the agent which makes decisions and the environment which the agents seeks to achieve a goal \cite{Sutton1}, there are four other aspects which build up every reinforcement learning system: a policy, reward signal, value function, and a model.

The policy is the set of rules which determines how an agent behaves at a given time. It is a mapping of the reward of experienced states to actions. When in experienced states, the aim is to select the action that would best achieve its goal \cite{GabrieleDe}. The policy of an agent can be a simple lookup table of states to actions and their corrosponding reward or involve a complex computation \cite{Sutton1}. 

The reward signal of an environment defines the problem an agent aims to solve. After every action performed on a state, the agent recieves a numerical value called reward. The aim of the agent is to maximize this reward value in the long term, as reward is an indication by the environment of good and bad decision making. The reward recieved is associated to the action chosen for the experienced state, which is the basis for altering the policy to increase the probability of choosing better actions in the same or similar states \cite{Sutton1}. 

While reward is a measure of how well an action is, in the given state, value is the long term reward of short-term actions. Value is the accumulative reward over a given time, which is more important when judging action choices. A set of actions that lead to a high amount of reward in the short-term is considered to be high in value. However, the same set of action in the long-term where the actions do not lead to increases in reward would have a low value. Despite the importance of value in long-term decision making and achieving the goal of the environment, value must be estimated by the agent over its lifetime. Therefore, value is the most important and influental component of reinforcement learning to making an optimal policy \cite{Sutton1}. 

The last essential aspect of reinforcement learning is the model of the environment. The model of the environment is an inferences of how the environment will behave. Given a state action pair, the model would be able to make a prediction of returned reward for the state-action pair and resulting next state given the action. Models are for forward planning and performing actions which will yield further high rewards, which are used in model-based methods of learning, contrasting from trial-and-error methods called model-free learning \cite{Sutton1}.