\section{Introduction}
The aim of this project is to develop a reinforcement learning (RL) agent to play Pokémon Red to compare the effectiveness of different RL algorithms and techniques to solving complex problem spaces. The pokemon red problem space is a videogame targetted at children but is complex due to its large action space and countless decisions which influnce future states.

% Do I need to add an abstract at the start?

\subsection{What is Reinforcement Learning}

Decision-making is a recurring activity that every individual faces in their everyday life, leading to short- or long-term consequences with differing levels of satisfaction. When making decisions in uncertain environments, humans have the ability to take previous experiences and apply them to new environments to make decisions grounded in knowledge. This is called sequential decision-making. In the context of machine learning, it refers to a decision-making agent in an observation and action loop, where after a series of actions, its performance can be measured \cite{francon2020effective}. Taking inspiration from biological learning systems, RL is the closest kind of machine learning that learns using observed data to influence the brain's reward system \cite{Sutton1}. 

RL is different from common traditional machine learning techniques as it learns "how to map situations to actions so as to maximize a numerical reward signal \cite{Sutton1}". Agents' requirement to learn through experience and actions performed on current states not only affect the present but also affect future states and actions are two characteristics that distinguish them from other forms of ML. It is also what makes Pokémon Red a suitable environment to apply this style of ML to. Initially, the agent has no understanding of the environment; however, through random action selection and the reward that it receives, it learns an understanding of the environment it is in. In RL, the agent is an AI that chooses actions by following a set of rules; this set of rules is called a policy. The action chosen by the policy is applied to the environment in which the agent is interacting. The place in which the decision-making agent performs actions and the entity that determines what is right and wrong are called the environment. The environment is everything the agent cannot control but is able to interact with. After the action has been applied to the environment, the change in the environment will be evaluated and return positive or negative feedback to the agent. This positive or negative feedback is called reward value. If the action chosen by the agent satisfied the goal of the environment, a positive reward would be returned. The agent’s aim is to maximize the amount of positive reward it can receive. Therefore, over a long period of time, the agent will learn the perfect set of actions that lead to the highest accumulative reward. 

RL is unique from other traditional AI because it is currently the closest form of natural intelligence \cite{Sutton1}. Through understanding mistakes and trying to maximize correct actions, it never stops learning. The agent is incentivized to maximize its reward and will aim to find actions that will yield more reward. This constant state, action, and reward loop is what teaches the agent improve by making small adjustments to the policy after every cycle. 

\subsection{Elements of Reinforcement Learning}

Other than the agent which makes decisions and the environment which the agents seeks to achieve a goal despite uncertainty \cite{Sutton1}, there are four other aspects which build up every RL system: a policy, reward signal, value function, and a model.

The policy is the set of rules which determines how an agent behaves at a given time. It is a mapping of experienced states to actions taken when in those experienced states, where the aim is to select the appropriate action to achieve its goal \cite{GabrieleDe}. The policy of an agent can be a simple lookup table of states to actions and their corrosponding reward or involve a complex computation \cite{Sutton1}. 

The reward signal of an environment defines the problem an agent aims to solve. After every action performed on a state, the agent recieves a numerical value called reward. The aim of the agent is to maximize this reward value in the long term, as reward is an indication by the environment of good and bad decisions. The reward recieved is associated to the action chosen for the experienced state, which is the basis for altering the policy to increase the probability of choosing better actions in the same or similar states \cite{Sutton1}. 

While reward is a measure of how well an action is, in the given state, value is the long term reward of short-term actions. Value is the accumulative reward over a given time, which is more important when judging action choices. A set of actions that lead to a high amount of reward in the short-term is considered to be high in value. However, in the long-term where the chosen actions do no lead to increases in reward would have a low value. Despite of value in achieving the goal of the environment, value must be estimated by the agent over its lifetime while reward is given by the environment. Therefore, value is the most important and influental component of RL to making an optimal policy \cite{Sutton1}. 

The last essential aspect of RL is the model of the environment. The model of the environment is an inferences of how the environment will behave. Given a state action pair, the model would be able to make a prediction of returned reward for the state-action pair and resulting next state given the action. Models are for forward planning and performing actions which will yield further high rewards, which are used in model-based methods of learning, contrasting from trial-and-error methods called model-free learning \cite{Sutton1}.



\subsection{Aims of the Project}
\subsection{Report Structure}
