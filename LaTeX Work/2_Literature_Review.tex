\section{Literature Review}

The literature review section will cover a few important topics necessary to understand the research being conducted. The video game environment will be introduced, along with why it was chosen for this research. Following will be an explanation of each algorithm chosen and why they were chosen.

\subsection{Introduction to Pokemon Red}

Pokémon Red is a role-playing game where the player's goal is to defeat the eight Gym Leaders and the Elite Four. The game is played from a top-down perspective and the player controls the actions of the main character. The player has to navigate the main character around the overworld, interact with non-player characters, and battle with wild Pokémon and other trainers. The game's combat is a turn-based battle system, where the player selects moves for their Pokémon to perform in battle. The game has a large number of items, moves, and abilities that the player can use to their advantage in battle. Therefore, large amount of variance makes each playthrough of the game unique while keeping goals consistent for each play through.

\subsection{Why Pokemon Red?}

RL has only been applied to every Atari game, where it surpassed the human benchmark for every game \cite{brockman2016openai}. However, these games lack long term randomness, where present actions influencing future states and future decisions. Every pokemon game takes at least 25 hours to complete, where every tiny decision made at every minute has an influence on the future \cite{howlongtobeat}. An example of this would be the decision of which pokemon to catch and train or which pokemon you start you journey with. The agent cannot defeat the game if it is able to navigate the map confidently but avoid battling, and on the other hand, the agent cannot complete the game if it chooses to only battle and not explore the map.

\subsection{Why choose RL?}

Within the field of machine learning, there are multiple different forms of learning that can be applied to the environment of pokemon red. However, due to the depth of the action space of the environment, RL is the best form of machine learning to explore how to find the optimal path to completing the game. 

The biggest drawback when using supervised or unsupervised learning is the dataset. The dataset would need someone playing the game for countless hours completing the game or reaching a checkpoint in the game before starting another episode of playing the game to provide a more varied dataset \cite{XanderSteenbrugge2019intro}. Not only is this method increcible slow, as humans can only play and operate at a certain speed, but the dataset would also never experience actions that are unnatural for a human to perform, as the dataset is bound to the actions a human would take with the human's preconception of how to play the game. This leads onto the other issue, where the dataset of human playing the game is bound by human constaints. Humans are naturally lazy and have short attention spans, which means when the person providing the training data knows one way to solve a puzzle, they are unlikely to experiment other methods in solving the puzzle \cite{XanderSteenbrugge2019intro}. Therefore, the agents trained on the human provided data are bound and limited by the performance of the human and will never find a more optimal path. 

RL is the solution to finding the set of actihroon to complete the game as it allows the agent to 'play' the game itself and explore the environment in a sped-up space to find the optimal path without the need of a human to show it how to play, while knowing what the goal is. 

\subsection{Deep Q Networks (DQN)}

DQN is a value based off-policy algorithm that learns by optimizing a value function to maximize reward in the long term. DQN was developed developed by combining RL techniques and deep neural networks at scale by enhancing the Q-Learning algorithm with deep neural networks and a technique called experience replay \cite{TFAgentsAuthors2023}.

Q-Learning is based on the Bellman equation, where the agent learns the optimal action-value function by iteratively updating the Q-values of the state-action pairs \cite{mnih2013playing}. The Q-value of a state-action pair is the expected return of taking the action in the current state, where the value of a state is the sum of the immediate reward and the discounted value of the future states \cite{bellman1958dynamic}. It is impractical to explore the Q-values of every state-action pair, as the state space of the environment is too large to computationally check \cite{mnih2013playing}. Instead, a function approximator such as a deep neural network is used to estimate unexperienced Q-values. 

Q-Learning is an off-policy algorithm, meaning that there is a seperate behavior policy for interacting with the environment. In DQN, the behavior policy is an epsilon-greedy policy. The episolon-greedy policy selects the action with the highest Q-value with a probability of $1 - epsilon$ and selects a random action with $epsilon$ probability \cite{TFAgentsAuthors2023}. The epsilon-greedy policy is used to balance the exploration and exploitation of the agent within the environment, as exploration and exploitation is what determines how often the agent should enter unexperienced observations and therefore fill out the Q-table. Balancing out exploration and exploitation is important as exploitation is dependent on the current known best set of  action and exploration is required to enter new states and discover states yielding higher reward \cite{TFAgentsAuthors2023}.

Value based algorithms, such as DQN, learn the environment by optimizing a value function, where it expects a high amount of value to be returned in the long term \cite{deepcheckRL}. Therefore, the learned agent will maximise future reward by making assumptions of rewards of future states yet to be experienced. This is an important step in value-based algorithms, as optimal action-value functions obey an important identity known as the Bellman equation \cite{mnih2013playing}. 

Value based algorithms are theoretically able to find the optimal policy, but have a few weaknesses when deployed for real world use outside of the simulated environment. One weakness of value based algorithms is the ability to generalise to new situations beyond the environment \cite{OdelTruxillo2023}. This is considered the reality gap, where there is a mismatch between the environment the policy is trained in and the real-world environment in which the agent is deployed in \cite{tobin2017domain}. Transfer learning is a technique that can be used to mitigate the reality gap, where the agent is trained in a simulated environment and then transferred to the real world environment \cite{OdelTruxillo2023}. However, transfer learning is not always possible, as the real world is complex to simulate accurately due to stochastic nature \cite{OdelTruxillo2023}. 

\subsection{Quantile Regression DQN (QR-DQN)}

Quantile Regression Deep Q Networks (QR-DQN) is an extension of DQN but instead aims to approximate the average future reward \cite{dabney2018distributional}. QRDQN, like DQN, is still a value based off-policy algorithm. In addition, it still uses the epsilon-greedy policy to balance exploration and exploitation. However, QRDQN was designed to be an improvement on DQN by focusing to learn certain quantiles of the distribution instead of trying to approximate the entire distribution of state-action pairs \cite{dabney2018distributional}. In doing so, QRDQN attempts to avoid the overestimation of Q-values that DQN is prone to by employing a model-based approach to finding the optimal policy.

\subsection{Proximal Policy Optimization (PPO)}

PPO is an on-policy, policy based algorithm trained to maximize reward in the future by choosing the best actions in each state \cite{deepcheckRL}. PPO was designed to improve the training stability of the policy by limiting the change made to the policy at each training epoch \cite{ThomasSimonini2022A2C}. On-policy algorithms, compared to off-policy algorithms, only use a singular policy to make decisions.

Due to how RL is dependent on the policy generating its own training dataset, the quality of the training data is dependent on the actions selected by the policy, which initially is via random action selectioin \cite{XanderSteenbrugge2019ppo}. Therefore. the distribution of observations and training is constantly changing as the policy is constantly being updated, which brings about instability during training. PPO aims to solve this problem by stabilizing the policy update during training by making small updates to the policy at the end of each episode \cite{XanderSteenbrugge2019intro}. This avoids the potential issue of the policy being pushed into a space where the next batch of data is learned under a poor policy further destabilizing the policy \cite{XanderSteenbrugge2019ppo}.

As mentioned in the Proximal Policy Optimization paper, "Policy gradient methods work by computing an estimator of the policy gradient and plugging it into a stochastic gradient ascent algorithm" \cite{schulman2017proximal}. This means that the policy, which is a neural network that takes the observed states as an input and suggests actions to take, is multiplied by the advantage function to determine the best action to take in a given state \cite{schulman2017proximal}. The advantage is the discounted sum of rewards that the agent expects to recieve in the future. The discount factor, is the scale between 0 and 0.99 used when calculating the discounted reward of how much the agent values future rewards \cite{XanderSteenbrugge2019ppo}. The higher the discount factor, the more the agent values future rewards. 

In comparison to value based algorithms, policy based algorithms are better at converging towards the optimum policy within stochastic environments \cite{mnih2015human}. This implies that policy based algorithms are better at adapting when deployed in a real world example outside of the environment because of the stochastic nature of reality. 

PPO aims to keep the data efficiency and reliable of algorithm TRPO, while only using first-order optimizations. The soltion to this problem was to make pessimistic estimations of the performance of the policy \cite{schulman2017proximal}. By limiting the amount the policy can update by per episode, it can stabilize the policy during training. Policy optimization is done by alternating between samplinig data from the policy and performing multiple epochs of optimization on the sampled data \cite{schulman2017proximal}. 

\subsection{Advantage Actor-Critic (A2C)}

A2C is an algorithm that combines aspects of policy gradient algorithms and value-based methods, where the policy, actor, selects actions and the value function, critic, is trained to estimate the expected reward \cite{mnih2013playing}. The actor follows a policy-based learning and takes the state as input and outputs the best possible action. This actor policy controls how the agents behaves by learning the optimal policy \cite{SergiosKaragiannakos2018}. The critic evaluates the selected action by the policy by computing its value using the value function. These two aspects of the algorithm get better doing their role by learning from each other and operate better together than as two methods seperately \cite{SergiosKaragiannakos2018}. 

As we learnt in DQN, value-based algorithms often use Q values to determine the best action to take in a given state. However, A2C uses the advantage function to determine the best action to take in a given state. Due to how Q values can be decomposed into the value function and the advantage function, the advantage value can be calculated to determine how good it is to be in the current state. \cite{SergiosKaragiannakos2018}. In other words, the advantage function calculates the extra reward recieved if the agent takes the action in the current state \cite{ThomasSimonini2022A2C}.

%! RESEARCH MONTE-CARLO SAMPLING

One advantage of combining value-based and policy-based algorithms by using Actor-Critic methods, is to help with stablize training and reduce the a mount of variance \cite{SergiosKaragiannakos2018}. When training a model in RL, the aim is to increase the probability of actions in a trajectory proportion to how high the return is. Such that if the returns a high, we will push the probability of the state action pair and if the the return is low, we will decrease the probability of the state action pair. The return value is calculated using Monte-Carlo sampling, where the agent trajectory is calculated using the discounted return to determine if the probability of the state action should increase or decrease. The issue with this is that in a stochastic environment, the return from the same state in different episodes can be significantly different \cite{ThomasSimonini2022A2C}. Actor critic methods help reduce the amount of variance by utilizing techniques such as clipped delayed policy updates and clipped double q learning \cite{padhye2023deep}.

\subsection{Algorithm Choice}

%! ADD MORE DETAIL

The algorithms chosen to be compared are Proximal Policy Optimization (PPO), Advanrage Actor-Critic (A2C), and Deep Q Networks(DQN). These algorithms are commonly used algorithms used in the field of RL for research purposes. These algorithms were chose because the environment used to conduct this research holds a discrete action space, where each action input is mapped to a possible button press on the gameboy. Therefore, the chosen algorithms had to support a discrete action space and not a continious action space.

In addition, these algorithms offer a spread of different RL algorithm techniques to determine which techniques are best suited for the multi-objective environment. Techniques such as comparing on and off policy algorithms, value and policy based algorithms, and how actor-critic methods, which use both value and policy based methods, compare to the other algorithms.

Another reason why these algorithms were chosen is because PPO was the original algorithm used to train on this environment which can be used as a baseline to compare the other algorithms to. 

\subsection{Similar works using RL in Pokemon}

Within the field of RL, similar works to what this research aims to achieve have been conducted. The use of RL in pokemon has been explored by researchers to find the optimal path to complete the game or create a model with an optimal battling strategy. From the surrounding research conducted, a large amount of research has been conducted on finding an optimal model for battling in pokemon. 

One detailed example of deep RL methods to train an agent to perform pokemon battling is by Kevin Chen and Elbert Lin, ``Gotta Train 'Em All: Learning to Play Pokemon Showdown with Reinforcement Learning''' \cite{chen2018gotta}. Kevin Chen and Elbert Lin used \textit{Pokemon Showdown}, the open-source recreation of the pokemon battling system, to train a RL agent to optimally battle against three different difficulties of opponents using random pokemon with different pokemon moves. Their research used PPO to train the agent to battle against opponent agents, which perform at three different levels of strategy. The first stage is an agent that performs random actions, the second stage is an agent that performs optimally with the pokemon currently in use, and the third stage is an agent that always chooses to use the strongest damaging move, which includes swapping to other more effective pokemon in its party. Due to number of possible pokemon to be used by the agent and the opponent, the environment used embeddings for each pokemon by grouping together similar pokemon to avoid the large action space. This was an effective way to accelerate the training of the agent by avoiding unexperienced states and to generalise past experiences. The agent was trained for 100 games, where the agent is rewarded with +1 for a victory and -1 for losing. The agent was able to defear the first stage and second stage agent, but had difficulty defeating the third stage agent. This was due to an action decision bias of choosing to use the third position pokemon move more often than the other action \cite{chen2018gotta}. This shows that despite pokemon battling being a turn-based game, it is a very complex and difficult environment to train an agent to perform single objective RL on, which will make it difficult to trian an agent to perform multi-objective RL on. 

Another example of pokemon battling being applied to the field of RL is by Akshay Kalose, Kris Kaya, and Alvin Kim, ``Optimal Battle strategy in Pokemon using Reinforcement Learning'' \cite{kalose2018optimal}. They used RL to find the optimal battling strategy in pokemon using model-free RL strategies. Unlike Kevin Chen and Elbert Lin's work, Akshay Kalose and their team created their own battle simulator and made it deterministic and limited the possible random pokemon to the first 151 from the first game \cite{kalose2018optimal}. These design choices, which differ from other research conducted is important in the context of this research. This is because this research uses the pokemon red environment, which is the first game released by the pokemon franchise and only has the first 151 pokemon. In addition, the choice of a deterministic environment limits the randomness of battling in the game, which is an important factor of RL because RL is designed to learn from the environment to be deployed in the real world and a deterministic environment is not a good representation of the real world. However, it is possible that the deterministic environment will accelerate training and not influence the stochastic nature of reality. Akshay Kalose and their team decided to use Q-Learning as their model-free algorithm to find the optimal policy, however had trouble training the agent to find the optimal policy after training for 5,000 battles against random action agents. The trained agent had a win rate of 65\%. This was due to the Q-Learning algorithm's in-ability to generalize similar states and the large action space the Q-Learning algorithm had to fill out using Q-values \cite{kalose2018optimal}. This shows that strategies which require the policy to fill out a Q-table are not effective in pokemon battling, as the action space is too large and the state space is too stochastic to fill out the Q-table. Therefore, making it further more difficult to train an agent to perform multi-objective RL.

%! continue from here
One project that was very similar to Kevin Chen and Elbert Lin's work by Jett Wang was, ``Winning at Pokemon Random Battles Using Reinforcement Learning'' \cite{wang2024winning}. The project also used Pokemon Showdown to train an agent to do pokemon battling trained against humans in a competitive battling setting online. However, Jett Wang used Monte Carlo Tree Searched informed by Actor-critic network trained using Proximal Policy Optimization. 

However, the work conducted by ... Not only does their work only focus on one aspect of the pokemon game which turn-based battling in video games is present in many other video games outside of the pokemon franchise, but also only focuses on researching different model-free strategies. However, the environment the research was conducted does not have a large enough search space and is not stochastic enough to justify application of RL techniques. This is because applying the same action to a set state between different episodes almost always lead to the same state, therefore making it possible for a rule based algorithm to optimize pokemon battling. 



Another similar work by Flaherty, Jimenez and Abbasi applies RL algorithms A2C and DQN to play Pokémon Red  \cite{flaherty2021playing}. Their work focuses on comparing A2C to DQN to see if RAM observations performed better than image observations for training agents \cite{flaherty2021playing}. Their work is  closely related to the research of this project, as the performance of A2c and DQN are being compared. However, my research aims to combine RAM and image observations during training to determine which style of algorithm is best suited for multi-objective RL. This research paper will extend upon their research by applying and comparing more algorithms and various RL techniques to find the optimal method to complete the game.

