\section{Literature Review}

The literature review section will cover a few important topics necessary to understand the research being conducted. The video game environment will be introduced, along with why it was chosen for this research. Following will be an explanation of each algorithm chosen and why they were chosen.

\subsection{Introduction to Pokemon Red}

Pokémon Red is a role-playing game where the player's goal is to defeat the eight Gym Leaders and the Elite Four. The game is played from a top-down perspective and the player controls the actions of the main character. The player has to navigate the main character around the overworld, interact with non-player characters, and battle with wild Pokémon and other trainers. The game's combat is a turn-based battle system, where the player selects moves for their Pokémon to perform in battle. The game has a large number of items, moves, and abilities that the player can use to their advantage in battle. Therefore, large amount of variance makes each playthrough of the game unique while keeping goals consistent for each play through.

\subsection{Why Pokemon Red?}

RL has only been applied to every Atari game, where it surpassed the human benchmark for every game \cite{brockman2016openai}. However, these games lack long term randomness, where present actions influencing future states and future decisions. Every pokemon game takes at least 25 hours to complete, where every tiny decision made at every minute has an influence on the future \cite{howlongtobeat}. An example of this would be the decision of which pokemon to catch and train or which pokemon you start you journey with. The agent cannot defeat the game if it is able to navigate the map confidently but avoid battling, and on the other hand, the agent cannot complete the game if it chooses to only battle and not explore the map.
 
In addition, other similar projects exist that apply RL to pokemon to find the optimal battling strategy by Kalose et al \cite{kalose2018optimal}. Their work focuses on finding an optimal pokemon battling strategy using model-free RL strategies \cite{kalose2018optimal}. Not only does their work only focus on one aspect of the pokemon game which turn-based battling in video games is present in many other video games outside of the pokemon franchise, but also only focuses on researching different model-free strategies. However, the environment the research was conducted does not have a large enough search space and is not stochastic enough to justify application of RL techniques. This is because applying the same action to a set state between different episodes almost always lead to the same state, therefore making it possible for a rule based algorithm to optimize pokemon battling. 

Another similar work by Flaherty, Jimenez and Abbasi applies RL algorithms A2C and DQN to play Pokémon Red  \cite{flaherty2021playing}. Their work focuses on comparing A2C to DQN to see if RAM observations performed better than image observations for training agents \cite{flaherty2021playing}. Their work is  closely related to the research of this project, as the performance of A2c and DQN are being compared. However, my research aims to combine RAM and image observations during training to determine which style of algorithm is best suited for multi-objective RL. This research paper will extend upon their research by applying and comparing more algorithms and various RL techniques to find the optimal method to complete the game.

\subsection{Why choose RL?}

Within the field of machine learning, there are multiple different forms of learning that can be applied to the environment of pokemon red. However, due to the depth of the action space of the environment, RL is the best form of machine learning to explore how to find the optimal path to completing the game. 

The biggest drawback when using supervised or unsupervised learning is the dataset. The dataset would need someone playing the game for countless hours completing the game or reaching a checkpoint in the game before starting another episode of playing the game to provide a more varied dataset \cite{XanderSteenbrugge2019intro}. Not only is this method increcible slow, as humans can only play and operate at a certain speed, but the dataset would also never experience actions that are unnatural for a human to perform, as the dataset is bound to the actions a human would take with the human's preconception of how to play the game. This leads onto the other issue, where the dataset of human playing the game is bound by human constaints. Humans are naturally lazy and have short attention spans, which means when the person providing the training data knows one way to solve a puzzle, they are unlikely to experiment other methods in solving the puzzle \cite{XanderSteenbrugge2019intro}. Therefore, the agents trained on the human provided data are bound and limited by the performance of the human and will never find a more optimal path. 

RL is the solution to finding the set of action to complete the game as it allows the agent to 'play' the game itself and explore the environment in a sped-up space to find the optimal path without the need of a human to show it how to play, while knowing what the goal is. 

\subsection{Proximal Policy Optimization (PPO)}

RL's training data is dependent on the policy generating its own training dataset, which is dependent on the training of its policy to provide valuable data \cite{XanderSteenbrugge2019ppo}. This means that the distribution of observations and training is constantly changing as the policy is constantly being updated, which brings about instability during training. Therefore, there is a need to stabilize the policy updates during training to avoid the policy being pushed into a space where the next batch of data is learned under a poor policy further destabilizing the policy \cite{XanderSteenbrugge2019ppo}.

\begin{quote}
    The idea with Proximal Policy Optimization (PPO) is that we want to improve the training stability of the policy by limiting the change you make to the policy at each training epoch: we want to avoid having too large of a policy update.
    \end{quote}
    \hspace*{\fill} \textit{Thomas Simonini, 2022}

PPO aims to keep the data efficiency and reliable of algorithm TRPO, while only using first-order optimizations. The soltion to this problem was to make pessimistic estimations of the performance of the policy \cite{schulman2017proximal}. By limiting the amount the policy can update by per episode, it can stabilize the policy during training. Policy optimization is done by alternating between samplinig data from the policy and performing multiple epochs of optimization on the sampled data \cite{schulman2017proximal}. 

PPO is a policy based algorithm, where the policy is trained to get the most reward in the future by doing actions in each state \cite{deepcheckRL}. In comparison to value based algorithms, policy based algorithms are better at converging towards the optimum policy within stochastic environments \cite{mnih2015human}. This implies that policy based algorithms are better at adapting when deployed in a real world example outside of the environment because of the stochastic nature of reality. 

As mentioned in the Proximal Policy Optimization paper, "Policy gradient methods work by computing an estimator of the policy gradient and plugging it into a stochastic gradient ascent algorithm" \cite{schulman2017proximal}. This means that the policy, which is a neural network that takes the observed states as an input and suggests actions to take, is multiplied by the advantage function to determine the best action to take in a given state \cite{schulman2017proximal}. The advantage is the discounted sum of rewards that the agent expects to recieve in the future. The discount factor, is the scale between 0 and 0.99 used when calculating the discounted reward of how much the agent values future rewards \cite{XanderSteenbrugge2019ppo}. The higher the discount factor, the more the agent values future rewards. 

\subsection{Deep Q Networks (DQN)}

DQN was developed developed by combining RL techniques and deep neural networks at scale by enhancing the Q-Learning algorithm with deep neural networks and a technique called experience replay \cite{TFAgentsAuthors2023}. DQN is a value based algorithms that attempts to learn the environment by optimizing a value function, where it expects a high amount of value to be returned in the long term \cite{deepcheckRL}. Therefore, the learned agent will maximise future reward by making assumptions of rewards of future states yet to be experienced. This is an important step in value-based algorithms, as optimal action-value functions obey an important identity known as the Bellman equation \cite{mnih2013playing}. 

Q-Learning is based on the Bellman equation, where the agent learns the optimal action-value function by iteratively updating the Q-values of the state-action pairs \cite{mnih2013playing}. The Q-value of a state-action pair is the expected return of taking the action in the current state, where the value of a state is the sum of the immediate reward and the discounted value of the future states \cite{bellman1958dynamic}. However, it is impractical to explore the Q-values of every state-action pair, as the state space of the environment is too large to computationally check \cite{mnih2013playing}. Instead, a function approximator such as a deep neural network is used to estimate unexperienced Q-values. 

Q-learning is an off-policy algorithm, meaning that there is a seperate behavior policy for interacting with the environment. In DQN, the behavior policy is an epsilon-greedy policy, where the agent selects the action with the highest Q-value with probability 1 - epsilon and selects a random action with probability epsilon \cite{TFAgentsAuthors2023}. The epsilon-greedy policy is used to balance the exploration and exploitation of the agent within the environment, as exploitation is dependent on current known best action and exploration is required to find an even better action.

Value based algorithms are theoretically able to find the optimal policy, but have a few weaknesses when deployed for real world use outside of the simulated environment. One weakness of value based algorithms is the ability to generalise to new situations beyond the environment \cite{OdelTruxillo2023}. This is considered the reality gap, where there is a mismatch between the environment the policy is trained in and the real-world environment in which the agent is deployed in \cite{tobin2017domain}. Transfer learning is a technique that can be used to mitigate the reality gap, where the agent is trained in a simulated environment and then transferred to the real world environment \cite{OdelTruxillo2023}. However, transfer learning is not always possible, as the real world is complex to simulate accurately due to stochastic nature \cite{OdelTruxillo2023}. 

Another weakness of value based algorithms is the environment they are trained in. Value based algorithms are able to converge towards the optimal policy by finding the set of actions which reward the most amount of value \cite{OdelTruxillo2023}. However, in some cases, the environment the agent is trained in must be fully observable, where the agent is able to see the entire state of the environment at every timestep. This will be a weakness for the agent because of its dependency on complete and whole information, as most real world environments are partially observable. When deployed to the real world, the agent will be unable to see the entire state of the environment at every time step and would be forced to make decisions on partial information \cite{dulac2021challenges}. 

\subsection{Advantage Actor-Critic (A2C)}

A2C is an algorithm that combines aspects of policy gradient algorithms and value-based methods, where the policy, actor, selects actions and the value function, critic, is trained to estimate the expected reward \cite{mnih2013playing}. The actor follows a policy-based learning and takes the state as input and outputs the best possible action. This actor policy controls how the agents behaves by learning the optimal policy \cite{SergiosKaragiannakos2018}. The critic evaluates the selected action by the policy by computing its value using the value function. These two aspects of the algorithm get better doing their role by learning from each other and operate better together than as two methods seperately \cite{SergiosKaragiannakos2018}. 

As we learnt in DQN, value-based algorithms often use Q values to determine the best action to take in a given state. However, A2C uses the advantage function to determine the best action to take in a given state. Due to how Q values can be decomposed into the value function and the advantage function, the advantage value can be calculated to determine how good it is to be in the current state. \cite{SergiosKaragiannakos2018}. In other words, the advantage function calculates the extra reward recieved if the agent takes the action in the current state \cite{ThomasSimonini2022A2C}.

%! RESEARCH MONTE-CARLO SAMPLING

One advantage of combining value-based and policy-based algorithms by using Actor-Critic methods, is to help with stablize training and reduce the a mount of variance \cite{SergiosKaragiannakos2018}. When training a model in RL, the aim is to increase the probability of actions in a trajectory proportion to how high the return is. Such that if the returns a high, we will push the probability of the state action pair and if the the return is low, we will decrease the probability of the state action pair. The return value is calculated using Monte-Carlo sampling, where the agent trajectory is calculated using the discounted return to determine if the probability of the state action should increase or decrease. The issue with this is that in a stochastic environment, the return from the same state in different episodes can be significantly different \cite{ThomasSimonini2022A2C}. Actor critic methods help reduce the amount of variance by utilizing techniques such as clipped delayed policy updates and clipped double q learning \cite{padhye2023deep}.

\subsection{Algorithm Choice}

%! ADD MORE DETAIL

The algorithms chosen to be compared are Proximal Policy Optimization (PPO), Advanrage Actor-Critic (A2C), and Deep Q Networks(DQN). These algorithms are commonly used algorithms used in the field of RL for research purposes. These algorithms were chose because the environment used to conduct this research holds a discrete action space, where each action input is mapped to a possible button press on the gameboy. Therefore, the chosen algorithms had to support a discrete action space and not a continious action space.

In addition, these algorithms offer a spread of different RL algorithm techniques to determine which techniques are best suited for the multi-objective environment. Techniques such as comparing on and off policy algorithms, value and policy based algorithms, and how actor-critic methods, which use both value and policy based methods, compare to the other algorithms.

Another reason why these algorithms were chosen is because PPO was the original algorithm used to train on this environment which can be used as a baseline to compare the other algorithms to. 