\section{Future Work}

One way in which this project can be extended is to go beyond completing the first gym and to complete the entire game as intended. However, as I mentioned before, this would require longer episode lengths as the game is expected to take a human 25 hours long to complete. Two ways in which this can be shortened down is through the use of save states at the end of each gym or by using offline learning. For this experiment, I only used online learning, where the dataset is generated by the agent as it plays the game. Offline learning would involve using a dataset that is generated by a human playing the game or another policy. This would allow the agent to learn from previous experiences and shortened the episode length required to complete the game. This could be implemented by saving the dataset generated by the agent, which just completed a gym to be fed into the agent that is going to defeat the next gym, thus making each gym a checkpoint of knoweldge for the agent. Another way in which this could be implemented is by using a save state at the end of each gym. The game was not intended for the player to complete it within one session of playing the game but meant to take advantage of saving the game and returning to it later. This could be implemented by saving the state of the game at the end of each gym and loading a fresh agent untrained agent to continue from that save state. However, this would require the agent to learn from scratch and not be able to use the knowledge gained from the previous gym, which would make the project more akin to multiple policies learning to play the game rather than a single policy. 