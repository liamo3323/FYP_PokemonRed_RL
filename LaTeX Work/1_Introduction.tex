\section{Introduction}

This project aims is to use reinforcement learning (RL) techniques to solve a complex multi-objective environment without the use of multiple single objective agents. The goal is to train multiple agents using different algorithms to compare each algorithm's ability to solve 'Pokémon Red' with the least amount of timesteps. 

Single objective RL has already been solved and implemented in many environments, such as Atari games, where a standard way to find the solution has been established \cite{brockman2016openai}. However, multi-objective environments are more complex and a standard solution has not been defined yet. This project explores the potential of popular single objective algorithms in solving multi-objective environments.

The problem space, Pokémon Red, was chosen due to it being a complex environment with an unfathomable search space, while also being a defined video game that was not created from scratch for demonstration purposes. Therefore, the problem space created for this project was not curated to demonstrate the potential of the agents but to test the agents in a real-world scenario through the use of a video game.

\subsection{What is Reinforcement Learning}

Decision-making is a recurring activity that every individual faces in their everyday life, leading to short- or long-term consequences with differing levels of satisfaction. When making decisions in uncertain environments, humans have are able to take previous experiences and apply them to new environments to make decisions grounded in knowledge. This is called sequential decision-making. In the context of machine learning, it refers to a decision-making agent in an observation and action loop, where after a series of actions, its performance can be measured \cite{francon2020effective}. Taking inspiration from biological learning systems, RL is the closest kind of machine learning that learns using observed data to influence the brain's reward system \cite{Sutton1}. 

RL is different from traditional forms of machine learning as it learns "how to map situations to actions-so as to maximize a numerical reward signal \cite{Sutton1}". Traditional forms of machine learning are provided a dataset to train on while training in RL is dependent on the information extracted from interacting within the environment. Another feature of RL, which other traditional learning methods lack, is continuous learning \cite{sreenivas2022safe}. When other forms of machine learning are deployed and ready to be used, the AI follows a static set of rules that it has learned. Instead, RL follows a similar set of rules, however, continues learning when deploy on the field. Therefore, it is possible for RL to make changes to its policy and adapt to differences between the environment and the deployed situation \cite{sreenivas2022safe}.

One way of understanding how RL agents learn to solve a problem from the interaction is via Markov decision processes \cite{Sutton1}.

\begin{quote}
MDPs are a classical formalization of sequential decision making,
where actions influence not just immediate rewards, but also subsequent situations, or
states, and through those future rewards. Thus MDPs involve delayed reward and the
need to tradeoff immediate and delayed reward.
\end{quote}
\hspace*{\fill} \textit{Sutton, 2018, p47.}

Agents' requirement to learn through experience and actions performed on current states not only affects the present but also affects future states and actions. Initially, agents have no understanding of the environment; however, through random action selection and the reward received, they learn an understanding of the environment. In RL, the agent is an AI that chooses actions by following a set of rules; this set of rules is called a policy. The place in which the decision-making agent performs actions and the entity that determines how good the actions are is called the environment. The environment is a simulation of the world, which is a representation of the task the agent aims to solve. After the action has been applied in the environment, the change in the environment will be evaluated it return a positive or negative numerical feedback to the agent. This positive or negative feedback is called the reward value. If the action chosen by the agent satisfied the goal of the environment, a positive reward would be returned. The agent aims to maximize the amount of positive reward received. Therefore, over a long period of time, the agent will learn the set of actions that leads to the highest accumulative reward, which should solve the problem presented in the environment. 

Other than the agent, which makes decisions within the environment by following a policy, the agent seeks to achieve a goal \cite{Sutton1}. Four other important aspects build up every RL system: a policy, reward signal, value function, and a model.

The policy is the set of rules which determines how an agent behaves at any given time. It is a mapping of experienced states to actions taken. When in a state that has been previously experienced or encountered, the aim is to select the corresponding action that best solves the overall goal \cite{GabrieleDe}. The policy of an agent can be a simple lookup table of states to actions and their corresponding reward or involve a complex computation \cite{Sutton1}. 

The reward signal of an environment defines the complete problem an agent aims to solve. After every action performed on a state, the agent receives a numerical value called reward. The aim of the agent is to maximize this reward value in the long-term, as reward is indicated by the environment of good or bad decision-making done by the agent. The reward received is associated with the action chosen for the experienced state, which is the basis for altering the policy to increase the probability of choosing better actions in similar states \cite{Sutton1}. 

While reward is a measure of how well an action is, in the given state, value is the long-term reward of short term actions. Value is the accumulative reward over a given time, which is important when judging action choices. A set of actions that lead to a high amount of reward in the short term is considered to be high in value. However, the same set of actions in the long-term where actions do not lead to increases in reward would have a low value. Despite the importance of value in long-term decision-making and achieving the goal of the environment, value must be estimated by the agent over its lifetime. Therefore, value is the most influential component of RL to making an optimal policy and the hardest to correctly estimate \cite{Sutton1}. 

The last essential aspect of RL is the model of the environment. The model of the environment is an inference of how the environment will behave. Given a state-action pair, the model would be able to predict returned reward for the state-action pair. Models are for forward planning and performing actions which will yield further high rewards in the future, which are used in model-based methods of learning, contrasting with trial-and-error methods called model-free learning \cite{Sutton1}.

In conclusion, to wrap up all the aspects of RL, the agent learns to solve the problem presented in the environment by following a policy, which is a set of rules that determine the action taken in a state. The agent aims to maximize the reward signal, which is a numerical value returned by the environment after an action is taken. The agent learns to maximize the reward signal by estimating the long-term value of the state-action pair. After enough iterations and training, the agent will be confident in the policy it has created and will be able to solve the problem presented in the environment, which is called the optimal policy.

\subsection{Multi-Objective Environments}

Currently, RL research has shown that it is effective at solving single-objective environments, where the goal is to maximize a single reward. This is because of RL's effectiveness at solving optimization problems. However, within multi-objective environments, multiple rewards must be maximized which may include sacrificing reward in one objective to maximize reward in another. The requirement to balance multiple objectives requires the RL agent to perform a cost-benefit analysis of the actions it takes. This is a complex problem as the agent must learn to balance the contrasting objectives and make decisions that are not only beneficial in the short term but also in the long-term. In order to find the optimal policy in solving the problem presented in the environment \cite{hayes2022practical}. 

Despite the potential of multi-objective environments to be applied to real-world problems, there is a lack of research in the area. In addition, as mentioned by Hayes, the current research in multi-objective environments are limited to a few algorithms and environments \cite{hayes2022practical}. This is in part due to single-objective methods being applied to multi-objective problems by nature, which either exclude the multi-objective aspect of the problem or summate the objectives into a reward value. This has resulted in a lack of research in the area of multi-objective environments, which leads to a lack of datasets and benchmarks to test the effectiveness of multi-objective algorithms and solutions \cite{hayes2022practical}.

\subsection{Experiment Environment - Pokémon Red}

Pokémon Red is a role-playing game developed by Game Freak and published by Nintendo in 1996 \cite{HubZ_1998}. Within the game, the player controls a character who navigates through the world of Pokémon catching, battling and training pet-like creatures called Pokémon. One of the game's main objectives is to defeat the 8 pokémon gym leaders. The gym leaders are scattered throughout the world, where the player must navigate in order to defeat them. The game is designed to be played on a handheld console, where the player can save and load their progress. Therefore, the game is designed to be played over a long period and not to be completed in a single sitting, which contrasts with the Atari games benchmarked by OpenAI. Moreover, the pokémon video game series is a series of games developed on handheld consoles marketed towards kids making it a simple game to complete, however, this does not equate to it being an easy task for AI to complete \cite{HubZ_1998}. 

None of the details of the game have been altered from the original release of the game to ensure that the agents are tested on a realistic scenario. This is to ensure that the agents are tested in a real-world scenario and not a curated environment. In addition, the results of each agent may differ due to the difficulty of the project and the complexity of the game.  

\subsection{Aims of the Project}

The aims of this project include the following:

\begin{itemize}
    \item Create a custom environment using the OpenAI Gymnasium framework to train the agents.
    \item Train the agents to collect the first of eight gym badges in Pokémon Red.
    \item Train multiple agents using different RL algorithms to compare each of their performance in the multi-objective environments.
    \item Hyperparameter tune each agent to ensure that each algorithm is performing at its best to the environment.
    \item Evaluate each model by comparing the cumulative reward, learning speed and stability.

\end{itemize}

Within the environment of pokémon red, the goal of the environment is to complete the game. However, due to the complexity of the game, and the minimum length of 25+ hours required to complete the game, a smaller more achievable goal was set \cite{howlongtobeat}. This was done in order to gain a strong understanding of how well each algorithm could potentially perform in a more achievable space. The goal of the agent for this research paper is to collect the first of eight gym badge. Within the game, collecting a pokémon badge is an essential step to completing the game, which is why it is a good gauge of the model's potential to complete the game. Despite this scaled-down goal, the agents will still need to learn to balance both navigation and battle in order to defeat the first gym leader.