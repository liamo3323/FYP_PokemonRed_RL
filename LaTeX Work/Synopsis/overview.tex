\documentclass{surrey_disso_style}

\raggedbottom

\usepackage[english]{babel} % English language/hyphenation


\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\usepackage{cleveref}

% numeric
\usepackage[style=ieee,sorting=none,backend=biber]{biblatex}
\addbibresource{references.bib}


\title{Reinforcement Learning in Pokémon Red to explore complex multi-reward environments}
\author{Liam O'Driscoll}
\urn{6640106}
\date{\today}
\degree{Bachelor of Science in Computer Science with a year in placement}
\supervisor{Sotiris Moschoyiannis}

\begin{document}
\maketitle



\subsection{Definitions}
\begin{itemize}
   \item [\ding{171}] Agent: The decision making mechanism recieving state information and performing chosen actions.
   \item [\ding{171}] Environemnt: The world in which the agent interacts with. 
   \item [\ding{171}] State: A representation of the environment at the current timestep.
   \item [\ding{171}] Timestep: A value that increments after each action has passed since the start of the episode.
   \item [\ding{171}] Episode: An instance of the environment that the agent is interacting with.
   \item [\ding{171}] Action: The choice made by the agent in response to the state.
   \item [\ding{171}] Reward: The return value when an action is applied to a state.
   \item [\ding{171}] Reward Function: The mechanism in the environment that indicates how well the selected action is to achieving the goal of the environment.
   \item [\ding{171}] Policy: The decision making mechanism within the agent that decides the best action to perform given the state.
\end{itemize}

\subsection{Abbreviation}
\begin{itemize}
   \item [\ding{171}] RL - Reinforcement Learning 
   \item [\ding{171}] AI - Artifical Intelligence
   \item [\ding{171}] ML - Machine Learning 
   \item [\ding{171}] RPG - Role Playing Game
   \item [\ding{171}] A2C - Ascynchronous Actor Critic
   \item [\ding{171}] DQN - Deep Q Network
\end{itemize}

\newpage
\section{Introduction}
The aim of this project is to develop a reinforcement learning agent to play Pokémon Red to compare the effectiveness of different styles of RL algorithms. RL is an area of machine learning (ML) where agents make decisions and perform actions on states to achieve a goal. 

\subsection{Aims}

\begin{itemize}
   \item [\ding{54}] The aim of this project is to develop a RL agent to play Pokémon Red to compare the effectiveness of different styles of RL algorithms and their effectiveness to learn complex reward functions.
\end{itemize}

\subsection{Objectives}

\begin{itemize}
   \item [\ding{169}] Research applications of RL to Pokemon and conduct a literature review on them
   \item [\ding{169}] Implement Pokémon Red game to be a suitable for training of different RL algorithms.
   \item [\ding{169}] Evaluate the performance of different RL algorithms used to train agents within the environment.
   \item [\ding{169}] Evaluate performance of agents to different forms of rewards functions.
   \item [\ding{169}] Recommend further developments to the project and applications to real world projects.
\end{itemize}

\section{Literature Review}
In RL, the decision making agent learns through experiences and 'trial and error'. Initially, it has a lack in understanding of the 
environment. However, through random action selection and the reward that it recieves, it is able to learn an understanding of the 
environemnt. The agent is incentivized to maximise its reward and will aim to find actions that will yield more reward. This constant
 state, action and reward loop is what helps the agent improve by altering the policy after every cycle. 
\par

RL is different to common traditional machine learning techiniques as it learns "how to map situations to actions-so as to maximize 
a numerical reward signal \cite{Sutton1}." Agent's requirement to learn through experience and actions performed on current states 
not only affect the present, but also affect future states and actions are two characteristics which distinguishes itself from other 
forms of ML. It is also what makes Pokémon Red a suitable environment to apply this style of ML to. 
\par

RL has only been applied to every Atari game, where it surpassed the human benchmark for every game \cite{brockman2016openai}. 
However, these games lack long term randomness and present actions influencing future states. The game 'Pokémon Red' is an RPG game 
filled with various puzzles, non-linear world and a large amount of variance making each play through of the game unique while also 
require achieving the same goals. In addition, the game has 2 states the players is constantly in, the player is either in an 
overworld where they control their movement on a map or they are in a battle with another individual, where they control the actions 
of their monster.
\par

Other similar projects which applies RL to find the optimal battling strategy by Kalose et al \cite{kalose2018optimal}. 
Their work focuses on one aspect of the game and does not have a large enough search space to justify application of RL techniques. 
Another similar work by Flaherty, Jimenez and Abbasi \cite{flaherty2021playing} applies RL algorithms A2C and DQN to play Pokémon 
Red. However, this piece of work does not go into enough detail about the comparison of different RL techniques to find the best
 method to train an agent to complete large complex environments with a large search space. I aim to extend their research in 
 applying RL to Pokémon Red by applying more algorithms and various techniques RL that I will go into more detail in section 3.
\par

I chose to apply RL to this environment, Pokémon Red, because of the benefits it holds during trianing and applications of this 
research. This version of the game has the ability to speed-up the environment which allows for more timesteps to be completed 
so the agent can experience more states. Another reason is because its complexity. The end goal of Pokémon Red is to defeat all the 
gym leaders and become the champion. However, to reach this goal the player must complete a series of smaller tasks which are not 
explicitly specified in the reward function. An example of this would be navigation a 2-dimensional plane, solving puzzles and
 performing pokemon battles along the way. Getting the agent to learn smaller tasks while completing the main goal of the environment 
 can be applied and extended to the real world. Compared to other forms of AI, RL never stops learning even when deployed, which makes
 it a very effective method to adapt to new environments outside of the simulation and constantly learn to improve itself. 

\section{Technical Overview}

For the environment, I plan on using a copy of the original game and having RL manually make inputs into the emulation of the game 
on my computer. This saves on a large amount of time manually recreating the game. Python has a gameboy emulator package called 'PyBoy'
which I can run the game on and speed up emulation \cite{pyboy}. In addition, there is built-in support for RL training.

The implementations of the algorithms that will be used will come from stablebaselines. Stablebaselines' implementation of the algorithms 
have been well documented and have the ability to change the weightings of parameters used. 

I will be using tensorboard to view the performance of the algorithms after training. Another reason why I am using stablebaselines'
implementation of the algorithms is because of the integration with tensorboard to view the performance of the trained agent in a graph
form.

When comparing algorithms, I will ensure that each algorithm being compared has an equal chance of performing at its best. I will 
hyperparameter tune each algorithm to find the best weightings for each algorithm's hyperparameter to obtain its best performance 
for comparison. Hyperparameter tuning is an additional step but has been found to greatly increase the algorithm's performance and 
efficiency, while also making comparison between algorithms more fair \cite{zhang2021importance}.

The first comparison of algorithms I will perform is value based methods to gradient descent. To compare the two different types of 
algorithms, I will use the algorithm A2C for gradient descent methods and PPO as value based methods. I will be aiming to compare each
algorithm's learning rate, average reward level, losses and when their learning stabilizes.

Another algorithm I plan on testing is the performance of DQN to changes in how much the agent values future rewards. By changing the 
weighting associated with reward from future states, the agent would perform differently because it would try to move towards states with 
long term value versus short term high reward.

\section{Workplan}


\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{8pt}

\begin{center}
\begin{tabular}{|p{6.8cm}|p{6.8cm}|}
   \hline
   \textbf{Risks} & \textbf{Solutions}\\
   \hline
   Issues with gettign the environment to work  & Give ample time to researching working environments\\
   \hline
   Algorithms not supported with environment action space (discrete/continious)  & Research a wide range of algorithms to be implemented\\
   \hline
   Agent unable to learn anything within the environment  & Change the weightings for the algorithms or change the reward function of the environment\\
   \hline
   Reward function not specific enough   & Research how other individuals implement their reward function and have multiple reward criterias to meet\\
   \hline
   All perform too smilarly making it difficult to compare  & Change the weightings of each algorithm to ensure differences or train for longer\\
   \hline


\end{tabular}
\end{center}


\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{8pt}

\begin{center}
\begin{tabular}{|p{1.6cm}|p{13cm}|}
   \hline
   \textbf{   Month} & \textbf{Goals}\\
   \hline
   October & \begin{itemize}
      \item Rough structure of the report has been made. 
      \item Papers surrounding the project have been read (e.g., similar projects, algorithms that will be explored and technologies to be implemented)
      \item Coding for the project is at its early stages.
      \item Project Synopsis completed and submitted.
   \end{itemize} \\
   \hline
   November & 
      \begin{itemize}
      \item Research and test which algorithms are applicable for comparison and applicable to project.
      \item Draft introduction completed with a basic explanation of RL and how it is suitable for my environment.
      \item Implementation of the Environment is complete
      \end{itemize}\\
   \hline
   December & \begin{itemize}
      \item Minimum viable product of code is achieved
      \item Alter reward functions to give different incentives
      \item Problem Analysis has been written
      \item Design documentation and choice has been started
   \end{itemize}\\
   \hline
   January & \begin{itemize}
      \item Hyperparameter train sets of agents per algorithm
      \item Train agents on different algorithms 
      \item Complete Design choice 
      \item Start evaluation of agents
   \end{itemize}\\
   \hline
   February & \begin{itemize}
      \item Any necessary extra agent training to be compelted
      \item First version of Report is at a Submittable state
   \end{itemize}\\
   \hline
   March & \begin{itemize}
      \item Debugging time for any potential issues 
      \item Review of draft report submission
   \end{itemize}\\
   \hline
   April & \begin{itemize}
      \item Consider completing Extension Objectives
      \item Final report completed 
   \end{itemize}\\
   \hline
   May & \begin{itemize}
      \item Extra time for debugging
      \item Last final checks on final version of report
   \end{itemize}\\
   \hline

\end{tabular}
\end{center}

\newpage


%\bibliography{references}
%\bibliographystyle{plain}

\section{References}
\printbibliography[heading=none]


\end{document}
