\documentclass{surrey_disso_style}

\raggedbottom

\usepackage[english]{babel} % English language/hyphenation


\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\usepackage{cleveref}

% numeric
\usepackage[style=ieee,sorting=none,backend=biber]{biblatex}
\addbibresource{references.bib}


\title{Reinforcement Learning in Pokémon Red to explore complex multi-reward environments}
\author{Liam O'Driscoll}
\urn{6640106}
\date{\today}
\degree{Bachelor of Science in Computer Science with a year in placement}
\supervisor{Sotiris Moschoyiannis}

\begin{document}
\maketitle



\subsection{Definitions}
\begin{itemize}
   \item [\ding{171}] Agent: The decision making mechanism recieving state information and performing chosen actions.
   \item [\ding{171}] Environemnt: The world in which the agent interacts with. 
   \item [\ding{171}] State: A representation of the environment at the current timestep.
   \item [\ding{171}] Timestep: A value that increments after each action has passed since the start of the episode.
   \item [\ding{171}] Episode: An instance of the environment that the agent is interacting with.
   \item [\ding{171}] Action: The choice made by the agent in response to the state.
   \item [\ding{171}] Reward: The return value when an action is applied to a state.
   \item [\ding{171}] Reward Function: The mechanism in the environment that indicates how well the selected action is to achieving the goal of the environment.
   \item [\ding{171}] Policy: The decision making mechanism within the agent that decides the best action to perform given the state.
\end{itemize}
\newpage
\section{Introduction}
The aim of this project is to develop a reinforcement learning agent to play Pokémon Red to compare the effectiveness of different styles of RL algorithms and their effectiveness. RL is an area of machine learning (ML) where agents make decisions and perform actions on states to achieve a goal. 

\subsection{Aims}

\begin{itemize}
   \item [\ding{54}] The aim of this project is to develop a RL agent to play Pokémon Red to compare the effectiveness of different styles of RL algorithms and their effectiveness to learn complex reward functions.
\end{itemize}

\subsection{Objectives}

\begin{itemize}
   \item [\ding{169}] Research applications of RL to Pokemon and conduct a literature review on them
   \item [\ding{169}] Implement Pokémon Red game to be a suitable for training of different RL algorithms.
   \item [\ding{169}] Evaluate the performance of different RL algorithms used to train agents within the environment.
   \item [\ding{169}] Evaluate performance of agents to different forms of rewards functions.
   \item [\ding{169}] Recommend further developments to the project and applications to real world projects.
\end{itemize}

\section{Literature Review}
In RL, the decision making agent learns through experiences and 'trial and error'. Initially, it has a lack in understanding of the environment. However, through random action selection and the reward that it recieves, it is able to learn an understanding of the environemnt. The agent is incentivized to maximise its reward and will aim to find actions that will yield more reward. This constant state, action and reward loop is what helps the agent improve by altering the policy after every cycle. 
\par

RL is different to common traditional machine learning techiniques as it learns "how to map situations to actions-so as to maximize a numerical reward signal \cite{Sutton1}." Agent's requirement to learn through experience and actions performed on current states not only affect the present, but also affect future states and actions are two characteristics which distinguishes itself from other forms of ML. It is also what makes Pokémon Red a suitable environment to apply this style of ML to. 
\par

RL has only been applied to every Atari game, where it surpassed the human benchmark for every game. However, these games lack long term randomness and present actions influencing future states. The game 'Pokémon Red' is an RPG game filled with various puzzles, non-linear world and a large amount of variance making each play through of the game unique while also require achieving the same goals. In addition, the game has 2 states the players is constantly in, the player is either in an overworld where they control their movement on a map or they are in a battle with another individual, where they control the actions of their monster.
\par

Other similar works include is by Kalose et al \cite{kalose2018optimal} which applies RL to find the optimal battling strategy. Their work focuses on one aspect of the game and does not have a large enough search space to effectively apply rl techniques. Another similar work by Flaherty, Jimenez and Abbasi \cite{flaherty2021playing} applies Rl algorithms A2C and DQN to play Pokémon Red. However, this piece of work does not go into enough detail about the comparison of different rl techniques to find the best method to train an agent to complete large complex environments with a large search space. I aim to extend their research in applying rl to Pokémon Red by applying more algorithms and various techniques rl that I will go into more detail in section 3.
\par

I chose to apply rl to this specific environment, Pokémon Red, because of the benefits it holds with trianing and applications of research. This version of the game has the ability to speed-up the environment which allows for more timesteps to be completed and for the agent to experience more states. Another reason I chose to apply rl to this environment is because its complexity. The end goal of Pokémon Red is to defeat all the gym leaders and become the champion. However, to reach this goal the player must complete a series of smaller tasks which are not explicitly specified in the reward function. An example of this would be navigation in a 2-dimensional plane and solving puzzles along the way. The fact that the agent is requried to learn complete these tasks while completing the main goal can be applied and extended to real world applications.

\section{Technical Overview}

\begin{itemize}
   \item Hyperparameter tuning to find optimal performance per experiment.
   \item Comparison of Gradient Descent and Value based models.
   \begin{itemize}
      \item Value based:
         \begin{itemize}
            \item Proximal Policy Optimization
         \end{itemize}
      \item Gradient Descent:
         \begin{itemize}
            \item Actor-Critic Methods: A2C 
            \item Deep Deterministic Policy Gradient
         \end{itemize}
   \end{itemize}
   \item Evaluating change in Q values to learning the optimal model using DQN
   \item Explore the benefit of applying meta learning.
\end{itemize}

\newpage
\section{Workplan}

\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{8pt}

\begin{center}
\begin{tabular}{|p{1.6cm}|p{13cm}|}
   \hline
   \textbf{Month} & \textbf{Goals}\\
   \hline
   October & \begin{itemize}
      \item Rough structure of the report has been made. 
      \item Papers surrounding the project have been read (e.g., similar projects, algorithms that will be explored and technologies to be implemented)
      \item Coding for the project is at its early stages.
      \item Project Synopsis completed and submitted.
   \end{itemize} \\
   \hline
   Novemebr & 
      \begin{itemize}
      \item Research and test which algorithms are applicable for comparison and applicable to project.
      \item Draft introduction completed with a basic explanation of RL and how it is suitable for my environment.
      \item Implementation of the Environment is complete
      \end{itemize}\\
   \hline
   December & \begin{itemize}
      \item Minimum viable product of code is achieved
      \item Alter reward functions to give different incentives
      \item Problem Analysis has been written
      \item Design documentation and choice has been started
   \end{itemize}\\
   \hline
   January & \begin{itemize}
      \item Hyperparameter train sets of agents per algorithm
      \item Train agents on different algorithms 
      \item Complete Design choice 
      \item Start evaluation of agents
   \end{itemize}\\
   \hline
   February & \begin{itemize}
      \item Any necessary extra agent training to be compelted
      \item First version of Report is at a Submittable state
   \end{itemize}\\
   \hline
   March & \begin{itemize}
      \item Debugging time for any potential issues 
      \item Review of draft report submission
   \end{itemize}\\
   \hline
   April & \begin{itemize}
      \item Consider completing Extension Objectives
      \item Final report completed 
      \item Time allocated for debugging or potential issues
   \end{itemize}\\
   \hline
   May & \begin{itemize}
      \item Last final checks on final version of report
      \item Time allocated for debugging or potential issues
   \end{itemize}\\
   \hline

\end{tabular}
\end{center}

\newpage


%\bibliography{references}
%\bibliographystyle{plain}

\section{References}
\printbibliography[heading=none]


\end{document}
