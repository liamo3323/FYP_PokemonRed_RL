\documentclass{surrey_disso_style}

% Write the approved title of your dissertation
\title{Reinforcement Learning in Pokémon Red to explore complex multi-reward environments}

% Write your full name, as in University records
\author{Liam O'Driscoll}
\urn{6640106}

% Write the month and year of your submission
\date{\today}

% Uncomment the line for the Degree you are registered for
\degree{Bachelor of Science in Computer Science with a year in placement}

% Write the full name of your supervisor, without any titles
\supervisor{Sotiris Moschoyiannis}

\raggedbottom

\begin{document}
\maketitle



\subsection{Definitions}
\begin{itemize}
   \item [\ding{171}] Agent: The decision making mechanism recieving state information and performing chosen actions.
   \item [\ding{171}] Environemnt: The world in which the agent interacts with. 
   \item [\ding{171}] State: A representation of the environment at the current timestep.
   \item [\ding{171}] Timestep: A value that increments after each action has passed since the start of the episode.
   \item [\ding{171}] Episode: An instance of the environment that the agent is interacting with.
   \item [\ding{171}] Action: The choice made by the agent in response to the state.
   \item [\ding{171}] Reward: The return value when an action is applied to a state.
   \item [\ding{171}] Reward Function: The mechanism in the environment that indicates how well the selected action is to achieving the goal of the environment.
   \item [\ding{171}] Policy: The decision making mechanism within the agent that decides the best action to perform given the state.
\end{itemize}
\newpage
\section{Introduction}
The aim of this project is to develop a reinforcement learning agent to play Pokémon Red to compare the effectiveness of different styles of RL algorithms and their effectiveness. RL is an area of machine learning (ML) where agents make decisions and perform actions on states to achieve a goal. 

\subsection{Aims}

\begin{itemize}
   \item [\ding{54}] The aim of this project is to develop a RL agent to play Pokémon Red to compare the effectiveness of different styles of RL algorithms and their effectiveness to learn complex reward functions.
\end{itemize}

\subsection{Objectives}

\begin{itemize}
   \item [\ding{169}] Research applications of RL to Pokemon and conduct a literature review on them
   \item [\ding{169}] Implement Pokémon Red game to be a suitable for training of different RL algorithms.
   \item [\ding{169}] Evaluate the performance of different RL algorithms used to train agents within the environment.
   \item [\ding{169}] Evaluate performance of agents to different forms of rewards functions.
   \item [\ding{169}] Recommend further developments to the project and applications to real world projects.
\end{itemize}

\section{Literature Review}
In RL, the agent learns through experiences and 'trial and error'. Initially, it has a lack in understanding of the environment. However, through random action selection and the reward that it recieves, it is able to learn an understanding of the environemnt. The agent is incentivized to maximise its reward and will aim to find actions that will yield more reward. This constant state, action and reward loop is what helps the agent improve by altering the policy after every cycle. 
\par

RL is different to common traditional machine learning techiniques as it learns "how to map situations to actions-so as to maximize a numerical reward signal." Agent's requirement to learn through experience and actions performed on current states not only affect the present, but also affect future states and actions are two characteristics which distinguishes itself from other forms of ML. It is also what makes Pokémon Red a suitable environment to apply this style of ML to.
\par

[MUST mention how important markov chains are and how they are applicable to pokemon!]

\section{Technical Overview}

\begin{itemize}
   \item Hyperparameter tuning to find optimal performance per experiment.
   \item Comparison of Gradient Descent and Value based models.
   \begin{itemize}
      \item Value based:
         \begin{itemize}
            \item Proximal Policy Optimization
         \end{itemize}
      \item Gradient Descent:
         \begin{itemize}
            \item Actor-Critic Methods: A2C 
            \item Deep Deterministic Policy Gradient
         \end{itemize}
   \end{itemize}
   \item Evaluating change in Q values to learning the optimal model using DQN
   \item Explore the benefit of applying meta learning.
\end{itemize}

\newpage
\section{Workplan}

\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{8pt}

\begin{center}
\begin{tabular}{|p{1.6cm}|p{13cm}|}
   \hline
   \textbf{Month} & \textbf{Goals}\\
   \hline
   October & \begin{itemize}
      \item Rough structure of the report has been made. 
      \item Papers surrounding the project have been read (e.g., similar projects, algorithms that will be explored and technologies to be implemented)
      \item Coding for the project is at its early stages.
      \item Project Synopsis completed and submitted.
   \end{itemize} \\
   \hline
   Novemebr & 
      \begin{itemize}
      \item Research and test which algorithms are applicable for comparison and applicable to project.
      \item Draft introduction completed with a basic explanation of RL and how it is suitable for my environment.
      \item Implementation of the Environment is complete
      \end{itemize}\\
   \hline
   December & \begin{itemize}
      \item Minimum viable product of code is achieved
      \item Alter reward functions to give different incentives
      \item Problem Analysis has been written
      \item Design documentation and choice has been started
   \end{itemize}\\
   \hline
   January & \begin{itemize}
      \item Hyperparameter train sets of agents per algorithm
      \item Train agents on different algorithms 
      \item Complete Design choice 
      \item Start evaluation of agents
   \end{itemize}\\
   \hline
   February & \begin{itemize}
      \item Any necessary extra agent training to be compelted
      \item First version of Report is at a Submittable state
   \end{itemize}\\
   \hline
   March & \begin{itemize}
      \item Debugging time for any potential issues 
      \item Review of draft report submission
   \end{itemize}\\
   \hline
   April & \begin{itemize}
      \item Consider completing Extension Objectives
      \item Final report completed 
      \item Time allocated for debugging or potential issues
   \end{itemize}\\
   \hline
   May & \begin{itemize}
      \item Last final checks on final version of report
      \item Time allocated for debugging or potential issues
   \end{itemize}\\
   \hline

\end{tabular}
\end{center}

% appendices
\appendix
%\include{appenda}
%\include{appendb}

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
