\section{Research Problem}

The aim of this project is to use RL techniques to solve a complex multi-objective environment without the use of multiple agents. The goal is to train multiple agents using different algorithms to compare their ability to navigate and complete the game environment, 'Pokémon Red', with the least number of timesteps. This section will go into detail about the environment the agent will be trained in, the need for multi-objective RL, and the benefits this research could bring to the field of RL.

\subsection{Background Information on the Environment}

The environment that the agent will be trained in is the game 'Pokémon Red', where the scope of this project will have the agents train to collect the first of eight total gym badges in the game. In the game, gym badges are collected after completing a battle with the gym leader, which is designed in a way to be a test before being able to progress to the next stage of the game. Along the way, the agent will come across manditory battles before reaching the first gym, navigate through unique looking zones, and interact with key non-player controlled characters to progress through the game. 

The agent is unable to navigate freely through out the environment of the game, as there will be obstacles in the way designed to block the agent from progressing until it a criteria has been met. These criterias include collecting a certain amount of badges, talking to key non-player controlled characters, or having key items in the agents inventory.  

Another obstacle that the agent will have to overcome is the need to use HMs. Within the game, items called TMs and HMs are used to teach pokemon new moves \cite{SerebiiTeam2016}. HMs are a specific move that can be used outside of battle to navigate through the environment, such as cutting down trees or surfing across water. Pokemon are unable to naturally learn HMs through leveling up via battling, therefore the agent would have to explore the environment to find the HMs and navigate the menus to teach the HM to a pokemon.

Within the environment, pokemon battling is similar to rock, paper, scissors but played out controlling pokemon. Each pokemon the agent can control within the environment, has a type, and each type has a weakness and strength to other types \cite{SerebiiTeam2016}. Each pokemon has a set of moves that it uses in battle, where different moves have different scales of strenth and influence on the battle. Battles are conducted in a turn-based manner, where the aim of the battle is to lower the opponent's pokemon's health to zero before they can lower your pokemon's health to zero. Moreover, after winning each battle, pokemon grow in the form of levels which increases their stats, which are numeric values that determins how strong they are in battle. 


\subsection{Environment Analysis}

These details about the game environment proves how complex the environment is and how small decisions made by the agent will have lasting effects and immediate consequences. Most progression within the environment is locked behind defeating a gym leader, therefore the agent must balance learning how to battle and navigate. 

Through out the agent's journey in the environemnt, the agent's pokemon have the opportunity to learn new moves, which can be used in battle. However, due to the nature of RL, the agent does not understand language semantics and will have to learn what its learnt move does through trial and error and apply the new knowledge in future episodes. Learning new moves is not necessary to complete the game, but it does increase the agents chance of winning battles and therefore increase the speed at which it completes the game. The possibility of each pokemon learning a range of different moves is very important to the goal of multi-objective RL, as there are many possible actions and decisions the agent can make in the environment to find the optimal path, but may never experience. However, with infinite time to train, theoretically the agent would be able to experience every possible state action pair and learn the optimal policy. 

When the agent reaches an obstacle that can only be overcome by using an HM, it will take time for it to understand the problem and overcome it. This is because the agent is dependent on the reward function to understand good and bad decision making, and so far it has not experienced a reward for using an HM to navigate further. In addition, to teach a pokemon an HM item requires navigating the menu and look through its bag to find the HM item, which is a complex task because it has not had to do this before for navigation or battling.

It is essential that there are manditory battles the agent has to complete before reaching the first gym, as the agent will need to learn how to battle in order to defeat the first gym leader. RL is very sensitive to hyperparameter such as the reward signal \cite{XanderSteenbrugge2019ppo}. Therefore, with a badly tuned set of parameters, the agents may be rewarded more for nagivation than time spent battling, which would result in the agent never learning to battle and not being able to defeat the first gym. On the other hand, if the reward signal is too extreme in the other direction, it is possible for the agent to only have the desire to battle and win every battle as it gets stronger and stronger at battling by constantly leveling up. In the context of multi-objective RL, having to balance two contrasting yet necessary rewards to competing the environment is the main challenge to finding the optimum policy. 

One problem with using a videogame as a RL environment is to determine how long an episode is. When openAI were able to use RL to complete every atari game, those games were designed to be quick, short and able to be completed with one sitting \cite{brockman2016openai}. However, Pokémon Red is a game that is designed to be played over a long period of time, with the minimum length of 25+ hours to complete the game \cite{howlongtobeat}. During training, an agent will update its model after compelting an episode, within a non-continious environment. However, the longer an episode is the fewer updates the agent will make to its model compared to an agent taking shorter episodes. In addition, there is a technical limitation on how long an episode is, as the longer an episode is the more RAM is required to keep the weights of the model in memory. Moreover, having a growing episode length would make it difficult to compare the performance of different models.


\subsection{Need for Multi-Objective RL} 

Multi-objective RL is an important area of research because of its real world benefits in compariosn to single-objective RL. In the real world, there are many problems with multiple objectives that need to be optimized and in some cases these objectives can be conflicting. For a human it would be possible to make a judgement decision to balance out the reward of both objectives, but this would be difficult with single-objective RL. In addition, environments with multiple objectives that are conflicting cannot be solved using two single-objective RL agents, as they would not be able to communicate with each other to make a decision that would be beneficial for both agents. %! FIND CITE

Another reason for the need of multi-objective RL is that it leads to better understanding of the environment in comparison to training two single-objective agents both assigned to handle their aspect of the environment. %? come back to this bit later

%! find some realife applications of MORL !!! 

\subsection{Potential Benefits}

Multi-Objective RL is still a very new area of research, and there is still a lot of potential for it to be used in real world applications. Most research done on multi-objective RL . A very interesting application of multi-objective RL was used to train a meta-policy, where a policy is simultaneously trained with multiple tasks sampled from a task distribution \cite{8968092}. The results of this paper showed that the meta-policy was able to learn a policy that was able to solve a range of different tasks, which was both optimal and computationally efficient. 

Despite the lack of real world examples of multi-objective RL, the most popular application of it is in the field of robotics. A paper by Vicki Young, Jumman Hossain and Nirmalya Roy compared the performance of single and multi-objective learning to enhance robotic navigation in their research paper \cite{young2023enhancing}. Despite their research being limited to the environment and not to an actual robot, they do intend to extend their research to a physical robot, which shows the potential and their belief in the multi-objective RL policy \cite{young2023enhancing}.