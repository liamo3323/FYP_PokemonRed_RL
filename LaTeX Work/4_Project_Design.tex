\section{Project Design}

This section has been broken into the following sub-sections: Environment Design, Agent Training Setup, and evaluation of the three trained models. This is to give a clear understanding of how the project is initally planned to be implemented and developed.

\subsection{Environment Design}

The environment will be using the offical Pokémon Red game and all interactions and necessary information will be taken from the game. Due to the original game being used to conduct the research, there is no need to recreate a representation of the game. The game will be emulated using Pyboy along side the OpenAI Gymnasium framework, which will allow the game to be interactable and useable as an environment. The environment will be designed to be a multi-objective environment, where the objectives of the environment would be to explore the game world and to conduct pokémon battling with the aim of being able to defeat the first gym. 

Within the environment, the agent will be able to interact with the game world through using keyboard mappings which have been mapped to gameboy buttons using the Pyboy emulator. Therefore, the agent will have complete control of actions such as moving, interacting with non-player controlled characters, and battling. This was planned to be designed in a way that the trained agent would have access to the same information a human playing the game. This would allow it to actually play the game on a real gameboy and follow its trained policy. However, that is beyond the scope of this research project.

The environment will be designed to be a partially observable environment. This follows the inital idea of it being as realistic to the actual game as possible. A partially observable environemnt means that the agent is only able to see a small portion of the game world at a time. This means that the agent does not have access to the entire game world at all times, and must explore the game world in order to gain information about it, where as a completely observable environment will give it access to information about the game the player normally would not have access to. Example of this would be the location of gym leaders and detailed statistics on enemy pokémon. 

As mentioned before, the agent takes a finite number of actions, where the actions are buttons on the gameboy. The agent will have access to up, down, left, and right for navigation, A and B for interacting with the game world, and start and select for interacting with the menu and its bag. Therefore, the environment will have a discrete action space as there is a limited number of actions the agent can take. 

The environment will be stochastic by having the game world be randomised at the start of each episode. This would allow the agent to learn a policy that is able to adapt to every episode and learn what actions will lead to statistically better outcomes despite the randomness.

The environment will be designed to have a continious or dense reward function, as the search space for completing the entire game within an episode is unreasonable. The agent would need to constantly rewarded for exploring new areas, defeating wild pokémon and leveling up, and defeating gym leaders. 

The environment will be episodic, where the agent will be able to explore the game world for a finite number of episodes. This allows the agent to be able to restart the game world and try a different approach if it ever gets stuck in an unplayable state.

\subsection{Agent Training Setup}

The agents will be trained on the environment using the stablebaselines3 implementation of the three selected algorithms. The amount of timesteps per episode will determine the amount of actions the agent can take to complete the goal of the environment, which is to defeat the first gym. However, the amount of timesteps per episode, which is currently unknown, would need to be determined through experimentation. The most likely determined outcome would be to train a model long enough that it would find a policy that would be able to defeat the first gym and use that as a point of reference to determine the amount of timesteps per episode. 

Tensorboard and Wandb will be used during training to ensure metrics and results are being tracked durinig training. This will allow for the comparison of the three algorithms after training is complete to be evaluated. 

Due to the nature of RL, the dataset that trains the model is generated by the agent interacting and applying actions to states. Therefore, there is no dataset external to the environment that is used to train the model. Offline learning can be utilized here to speed up the learning rate of agents by applying the experience of a previous agent on the same environment to the current agent \cite{Sutton1}. However, this is beyond the scope of this research project and has the possibility of introducing bias to the model and is not recommended for research purposes.

To solve the issue of the agent's training being dependent on the rate at which it is able to interact with the environemnt, the agent will be trained on the same policy in parallel to maximize the amount of experience it can gain within a feasible amount of time. There will not be issues with agents exploring the same regions and being sample inefficient because navigation is such a complex task that having multiple agents explore the same regions will build confidence in the policy being trained.

\subsection{Evalaution}

As mentioned before, Tensorboard and Wandb will be used to keep track of metrics during training and to plot graphs using these metrics. These two tools will be used to visualize the training process and to compare the three algorithms.

A range of different metrics provided and tracked on tensorboard along with previewing recordings of each agent playing the game will be used to compare the performance of the three trained models. The metrics to be used are: the average reward per episode, the cumulative reward, the average loss, and policy gradient. These metrics will be used to compare the performance of the three trained models and to determine which algorithm is the most effective at training the agent to complete the game.