\section{Project Design}

Due to the nature of the project being a reinforcement learning research project, I have broken down this section into the following sub-sections: Environment Design, and Agent Training Setup. This is to give a clear understanding of how the project is initally planned to be implemented and developed.

\subsection{Environment Design}

The environment will be using the offical Pok√©mon Red game and all interactions and necessary information will be taken from the game. Due to the original game being used to conduct the research, there is no need to recreate a representation of the game. The game will be emulated using Pyboy along side the OpenAI Gymnasium framework, which will allow the game to be interactable and useable as an environment. The environment will be designed to be a multi-objective environment, where the objectives of the environment would be to explore the game world and to conduct pokemon battling with the aim of being able to defeat the first gym. 

Within the environment, the agent will be able to interact with the game world through using keyboard mappings which have been mapped to gameboy buttons using the Pyboy emulator. Therefore, the agent will have complete control of actions such as moving, interacting with non-player controlled characters, and battling. This was planned to be designed in a way that the trained agent would have access to the same information a human playing the game. This would allow it to actually play the game on a real gameboy and follow its trained policy. However, that is beyond the scope of this research project.

The environment will be designed to be a partially observable environment. This follows the inital idea of it being as realistic to the actual game as possible. A partially observable environemnt means that the agent is only able to see a small portion of the game world at a time. This means that the agent does not have access to the entire game world at all times, and must explore the game world in order to gain information about it, where as a completely observable environment will give it access information about the game the player normally would not have access to. Example of this would be the location of gym leaders and statistics on enemy pokemon and their level. 

As mentioned before, the agent is only abel to take a finite number of actions, where the actions are the same as the gameboy buttons. The agent will have access to up, down, left, and right for navigation, A and B for interacting with the game world, and start and select for interacting with the menu and its bag. Therefore, the environment will have a discrete action space as there is a limited number of actions the agent can take. 

The environment will be stochastic by having the game world be randomised at the start of each episode. This would allow the agent to learn a policy that is able to adapt to every episode and learn what actions will lead to statistically better outcomes despite the randomness.

The environment will be designed to have a continious or dense reward function, as the search space for completing the entire game within an episode is unreasonable. The agent would need to constantly rewarded for exploring new areas, defeating wild pokemon and leveling up, and defeating gym leaders. 

The environment will be episodic, where the agent will be able to explore the game world for a finite number of episodes. This allows the agent to be able to restart the game world and try a different approach if it ever gets stuck in an unplayable state.

\subsection{Agent Training Setup}

The agents will be trained on the environment using the stablebaselines3 implementation of the three selected algorithms. The amount of timesteps per episode will determine the amount of actions the agent can take to complete the goal of the environment, which is to defeat the first gym. However, the amount of timesteps per episode is currently unknown would need to be determined through experimentation. The most likely determined outcome would be to train a model long enough that it would find a policy that would be able to defeat the first gym and use that as a scale to determine the amount of timesteps per episode. 