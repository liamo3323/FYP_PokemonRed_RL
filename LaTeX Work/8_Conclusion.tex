\section{Conclusion}

Within this section, the research project will be concluded by summarizing and reflecting on what worked well within the project, the challenges that I faced during the project, any improvements I would make and the future work to extend this research prroject.

\subsection{What worked well?}

During the research project, there were many aspects that worked well and contributed to the success of the project. However, one of the most useful tools that was taught during my 2nd year of university was the use of git version control. Git allowed me to work on the project on multiple different machines and keep a history of any changes. This was especially important as the project requried a lot of trial and error to get the algorithms to work correctly. In addition, when running the experiment on my main desktop, I was unable to use the computer for anything else, therefore being able to work from a secondary device allowed me to make better use of the available time.  

Another really important skill that I was taught during my year long placement was how to use tensorboard and evaluate the effectiveness of the model and the performance of each trained agenet. Tensorboard logging all of the information of the agent as it trained made it easy to see the progress of the agent and to determine if the agent was learning or not, as well as creating graphs to visualize the data for the report. Moreover, knowing what values to expect when evaluating a model made coming to a final conclusion more straight forward.  

The use of the OpenAI gym environment and the stablebaselines library written in Python were also very useful for this research project. In my final year of univeristy, python was used very extensively for all of the modules that I took, therefore a lot of transferable skills were able to be applied to this research project. In addition, the OpenAI gym environment was very easy to use and the supprot with stablebaselines3's implementation of the algorithms used for the experiment meant that I was able to focus more on the individuals details of the environment and worry less about writing the algorithms from scratch. 

Something else that helped with the most time consuming aspect of the research project, which was training the agent, was to use my own hardware. I was able to use my main desktop to train the agent through out the day and night, which meant that I had more control over the amount of time that the agent was training. This potentially avoided any issues with the agent not training or any downtimes with any alterantive services I would have used. In addition, it was also more cost effective in the long-term to use my own hardware.

\subsection{Challenges}

- The reward function was difficult to implement and required a lot of trial and error to get right
- Number of timesteps for the experiment was difficult 
- Steps per episode 
- Getting the agent to balance navigation and battling 
- The amount of time it took to determine the effectiveness of reward values

\subsection{Improvements}

If the research experiment were to be redone or extended in the short-term, many inital decisions and implementations would be done differently after gaining more knowledge and experience completing the experiment. 

\section{Future Work}

One way in which this project can be extended is to go beyond completing the first gym and to complete the entire game as intended. However, as I mentioned before, this would require longer episode lengths as the game is expected to take a human 25 hours long to complete. The game is not intended to be played in a singular session but to be played over multiple sessions. Two ways in which this can be shortened down is through the use of save states at the end of each gym or by using offline learning. 

For this experiment, I only used online learning, where the dataset is generated by the agent as it plays the game. However, offline learning would involve using a dataset before interacting with the environment. This dataset is generated by a human or another trained policy playing the game. This would allow the agent to learn from previous experiences and shortened the total training time as it does not require to start from zero previous knowledge. In addition, this technique could be implemented by saving the dataset generated by the agent, which just completed a gym, to be fed into the next agent that is going to defeat the next gym. In turn, making each gym a checkpoint of knoweldge for the agent. 

Another way in which the project can be extended to complete the entire game, would be to implement a checkpoint system at the end of each gym. Therefore, each agent would be tasked with getting from their starting position to completinig the next gym. This would be implemented by saving the game state when a gym badge is collected and stopping the training. A new untrained agent would then be loaded to continue from that save state. The downside to this method would be that it requires the agent to learn from scratch each time and not be able to use knowledge gained from the previous gym, which would make the project more akin to multiple policies learning to play the game rather than a single policy. In addition, requiring each agent to learn from scratch would increase training time, as the navigation and battling difficulty within the game is designed to increase as the player progresses through the game.

One way in which I wish this project could be extended or improved upon, would be to rewrite the reward function to be more generalized and less currated to the specific environment. Due to the similar nature of each generation of pokemon games, for example pokemon red, blue, yellow, gold, silver, etc, the reward function and get observation function could be rewritten to allow for a trained model to be used on any of the pokemon games. This would be a great example of showing the effectiveneses of RL in transfer learning, where a model trained on one environment can be used on another with similar goals. However, the reward function and observation function are too specific to the environment and rely on RAM readings to collect key information about the game state. All of the necessary information is theoretically available on the screen, such as the health of the pokemon, the level of the pokemon, the level of the enemy pokemon, etc, therefore it would be possible to have an external program read the necessary screen information and feed it into the environment. This would allow the agent to be trained on any pokemon game, to be deployed on any pokemon game. Moreover, this could be applied to all similar turn-based role-playing games. 

Lastly, for this research project I wish each algorithm was hyperparameter-tuned so that the results could be compared fairly and compared when at their best performance. However, it was not possible to do this due to the time constraints of the project and the fact that it would take 5-6 hours to train the agent once. Hyperparameter-tuning is a method to determine which hyperparameters for each algorithm would be the best for the environment, which involves training the agent multiple times with different hyperparameters to determine which is the best. This allows for a fair comparison but would require countless hours of training time for each value that has to be tested, for each of the three algorithms. 