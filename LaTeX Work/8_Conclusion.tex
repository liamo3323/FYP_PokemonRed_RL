\section{Conclusion}

Within this section, the research project will be concluded by summarizing and reflecting on what worked well within the project, the challenges that I faced during the project, any improvements I would make and the future work to extend this research project.

\subsection{Meeting the aims}

In conclusion, I believe that the work conducted in this research has met the project aims to a high degree. This project aimed to create a custom gymnasium framework environment will be made for the agents to train on, which was used to train the agents to get the results. Furthermore, the agent will be trained in the custom environment to collect the first gym badge, which was only successfully done by the PPO algorithm but achieved nonetheless. Furthermore, this project aimed to train multiple agents using different RL algorithms, which were met as PPO, DQN, and QRDQN. In addition, the project aimed to hyperparameter-tune tuned the algorithms to ensure that each algorithm is performing at its best in the environment, which was not met due to time and resource constraints. Moreover, this project will evaluate the trained agents by comparing the cumulative reward, learning speed and stability, which was covered in the previous section. 

\subsection{What worked well?}

During the research project, there were many aspects that worked well and contributed to the success of the project. However, one of the most useful tools that was taught during my 2nd year of university was the use of git version control. Git allowed me to work on the project on multiple different machines and keep a history of any changes. This was especially important as the project required a lot of trial and error to get the algorithms to work correctly. In addition, when running the experiment on my main desktop, I was unable to use the computer for anything else, therefore being able to work from a secondary device allowed me to make better use of the available time.  

Another really important skill that I was taught during my year-long placement was how to use tensorboard and evaluate the effectiveness of the model and the performance of each trained agent. Tensorboard logging all of the information of the agent as it trained made it easy to see the progress of the agent and to determine if the agent was learning or not, as well as creating graphs to visualize the data for the report. Moreover, knowing what values to expect when evaluating a model made coming to a conclusion more straightforward.  

The use of the OpenAI gym environment and the stablebaselines library written in Python were also very useful for this research project. In my final year of university, python was used very extensively for all of the modules that I took, therefore a lot of transferable skills were able to be applied to this research project. In addition, the OpenAI gym environment was very easy to use and the support with stablebaselines3's implementation of the algorithms used for the experiment meant that I was able to focus more on the individual details of the environment and worry less about writing the algorithms from scratch. 

Something else that helped with the most time-consuming aspect of the research project, which was training the agent, was to use my own hardware. I was able to use my main desktop to train the agent throughout the day and night, which meant that I had more control over the amount of time that the agent was training. This potentially avoided any issues with the agent not training or any downtimes with any alternative services I would have used. In addition, it was also more cost-effective in the long term to use my own hardware.

\subsection{Challenges}

While conducting this research, quite a few issues arose that made the project more difficult to complete but were still able to be solved. The largest issue and main objective of this research were to find the correct reward function so that it was feasible for the agent to reach the goal of the first gym in order to compare their performances. The reward function of the multi-objective environment was difficult to implement correctly by balancing battling and navigation. The correct values were eventually discovered after countless agents were trained and tested, all of which can be found in the evaluation section of the report. However, being able to train an agent once during the day and once during the night sped up this process. 

The difficulty of tuning the reward function was contributed by the initial reward function that was provided by the environment. The environment had very simple rewards that were not enough to train an agent to complete the first gym, it was designed to constantly train a singular agent till it eventually reached the goal. For example, the reward to encourage battling was static and would stop increasing the amount of reward given to the agent after reaching a certain level, which would result in the agent stopping to battle after reaching a certain point in the game. Another example was the reward for navigation, which would give a reward for every new screen. However, this could be exploited by the agent by navigating through menus and not actually moving the character in the game. Instead, a reward would only be given when a new screen is experienced and coordinates are updated. 

In addition, finding the correct number of total timesteps for the experiment was also difficult because it needed the value function to be corrected before further experimentation on an appropriate number of timesteps could be conducted. Initially, a value of 20 million timesteps was chosen but was later increased to 40 million to ensure that the agents had enough time to learn the environment and reach the first gym more consistently. However, it was a time-consuming task and required trial and error and lots of training time.

Another challenge that was faced during the research project was getting the initial algorithm script to work correctly. There were multiple examples of the algorithms being implemented in the stablebaselines3 library working with gym environments, however, the environment for this research was a custom gym environment and had to support multiple agents training on multiple instances of the environment, which is an added challenge. 

\subsection{Improvements}

If the research experiment were to be redone or extended in the short term, many initial decisions and implementations would be made differently after gaining more knowledge and experience completing the experiment. 

The first improvement would be to find a method to shorten the training time of the agent. The agent took 5-6 hours to train, which meant that any changes to the environment took a long time to test. One way in which this could be shortened is to create a hyper accurate representation of the environment on a smaller scale without the use of an emulation. This would allow for the agent to be trained in a smaller environment allowing for less RAM to be used to load the ROM. In addition, removing the dependency on the Gameboy emulator would allow for the environment to be further sped up. 

Another way in which I would improve this project would be to make the algorithm comparison more fair. The algorithms were not hyperparameter-tuned, which meant that the results were not comparable when at their best performance. Hyperparameter tuning would take a long amount of time to conduct but if more time were available, it would be a great way to compare the algorithms at their best performance. On the other hand, algorithms were not trained for the same amount of episodes, which is another way the algorithms were not compared fairly. The algorithms were trained for the same amount of timesteps, but due to different numbers of instances of agents training on the environment, the number of episodes completed was different. This was highlighted in the Evaluation section.

\newpage

\section{Future Work}

One way in which this project can be extended is to go beyond completing the first gym and complete the entire game as intended. However, as I mentioned before, this would require longer episode lengths as the game is expected to take a human 25 hours to complete. The game is not intended to be played in a singular session but to be played over multiple sessions. Two ways in which this can be shortened down is through the use of save states at the end of each gym or by using offline learning. 

For this experiment, I only used online learning, where the dataset is generated by the agent as it plays the game. However, offline learning would involve using a dataset before interacting with the environment. This dataset is generated by a human or another trained policy playing the game. This would allow the agent to learn from previous experiences and shorten the total training time as it does not require to start from zero previous knowledge. In addition, this technique could be implemented by saving the dataset generated by the agent, which just completed a gym, to be fed into the next agent that is going to defeat the next gym. In turn, this makes each gym a checkpoint of knowledge for the agent. 

Another way in which the project can be extended to complete the entire game would be to implement a checkpoint system at the end of each gym. Therefore, each agent would be tasked with getting from their starting position to completing the next gym. This would be implemented by saving the game state when a gym badge is collected and stopping the training. A new untrained agent would then be loaded to continue from that save state. The downside to this method would be that it requires the agent to learn from scratch each time and not be able to use the knowledge gained from the previous gym, which would make the project more akin to multiple policies learning to play the game rather than a single policy. In addition, requiring each agent to learn from scratch would increase training time, as the navigation and battling difficulty within the game is designed to increase as the player progresses through the game.

One way in which I wish this project could be extended or improved upon would be to rewrite the reward function to be more generalized and less curated to the specific environment. Due to the similar nature of each generation of pokémon games, for example, pokémon red, blue, yellow, gold, silver, etc, the reward function and get observation function could be rewritten to allow for a trained model to be used on any of the pokémon games. This would be a great example of showing the effectiveness of RL in transfer learning, where a model trained on one environment can be used on another with similar goals. However, the reward function and observation function are too specific to the environment and rely on RAM readings to collect key information about the game state. All of the necessary information is theoretically available on the screen, such as the health of the pokémon, the level of the pokémon, the level of the enemy pokémon, etc, therefore it would be possible to have an external program read the necessary screen information and feed it into the environment. This would allow the agent to be trained on any pokémon game, to be deployed on any pokémon game. Moreover, this could be applied to all similar turn-based role-playing games. 