\section{Conclusion}

Within this section, the research project will be concluded by summarizing and reflecting on what worked well within the project, the challenges that I faced during the project, any improvements I would make and the future work to extend this research prroject.

\subsection{What worked well?}

- Git version control for working on multiple machines to run the experiment as often as possible
- Tensorboard and wandb for vizualizing 
- OpenAI gym env + stablebaselines implementation 
- Investing in using my own hardware maximizing the amount of trianing time 

\subsection{Challenges}

- The reward function was difficult to implement and required a lot of trial and error to get right
- Number of timesteps for the experiment was difficult 
- Steps per episode 
- Getting the agent to balance navigation and battling 
- The amount of time it took to determine the effectiveness of reward values

\subsection{Improvements}

If the research experiment were to be redone or extended in the short-term, many inital decisions and implementations would be done differently after gaining more knowledge and experience completing the experiment. 

\section{Future Work}

One way in which this project can be extended is to go beyond completing the first gym and to complete the entire game as intended. However, as I mentioned before, this would require longer episode lengths as the game is expected to take a human 25 hours long to complete. The game is not intended to be played in a singular session but to be played over multiple sessions. Two ways in which this can be shortened down is through the use of save states at the end of each gym or by using offline learning. 

For this experiment, I only used online learning, where the dataset is generated by the agent as it plays the game. However, offline learning would involve using a dataset before interacting with the environment. This dataset is generated by a human or another trained policy playing the game. This would allow the agent to learn from previous experiences and shortened the total training time as it does not require to start from zero previous knowledge. In addition, this technique could be implemented by saving the dataset generated by the agent, which just completed a gym, to be fed into the next agent that is going to defeat the next gym. In turn, making each gym a checkpoint of knoweldge for the agent. 

Another way in which the project can be extended to complete the entire game, would be to implement a checkpoint system at the end of each gym. Therefore, each agent would be tasked with getting from their starting position to completinig the next gym. This would be implemented by saving the game state when a gym badge is collected and stopping the training. A new untrained agent would then be loaded to continue from that save state. The downside to this method would be that it requires the agent to learn from scratch each time and not be able to use knowledge gained from the previous gym, which would make the project more akin to multiple policies learning to play the game rather than a single policy. In addition, requiring each agent to learn from scratch would increase training time, as the navigation and battling difficulty within the game is designed to increase as the player progresses through the game.

One way in which I wish this project could be extended or improved upon, would be to rewrite the reward function to be more generalized and less currated to the specific environment. Due to the similar nature of each generation of pokemon games, for example pokemon red, blue, yellow, gold, silver, etc, the reward function and get observation function could be rewritten to allow for a trained model to be used on any of the pokemon games. This would be a great example of showing the effectiveneses of RL in transfer learning, where a model trained on one environment can be used on another with similar goals. However, the reward function and observation function are too specific to the environment and rely on RAM readings to collect key information about the game state. All of the necessary information is theoretically available on the screen, such as the health of the pokemon, the level of the pokemon, the level of the enemy pokemon, etc, therefore it would be possible to have an external program read the necessary screen information and feed it into the environment. This would allow the agent to be trained on any pokemon game, to be deployed on any pokemon game. Moreover, this could be applied to all similar turn-based role-playing games. 

Lastly, for this research project I wish each algorithm was hyperparameter-tuned so that the results could be compared fairly and compared when at their best performance. However, it was not possible to do this due to the time constraints of the project and the fact that it would take 5-6 hours to train the agent once. Hyperparameter-tuning is a method to determine which hyperparameters for each algorithm would be the best for the environment, which involves training the agent multiple times with different hyperparameters to determine which is the best. This allows for a fair comparison but would require countless hours of training time for each value that has to be tested, for each of the three algorithms. 