\section{Conclusion}


(one of the examples issues that arose during training)

Due to the environment's large search space, it was feasibly impossible to expect the agent to complete the game nor complete the first gym via random actions, which is why a sparse reward is needed to guide the agent by feeding it small amounts of reward to converge towards the optimal policy. The sparse reward for the Pokemon Red environment was implemented using a mix of on-screen information and RAM readings, which is reward shaping that would only work for Pokemon Red. Therefore, it would be impossible to use the same trained policy for other games without retraining the agent on a more suitable environment. This is one issue with reinforcement learning, as the field of RL is not mature enough to be able to create policies that can be transferred to other environments without retraining.